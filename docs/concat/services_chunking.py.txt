# ==========================================
# GROUPE : services_chunking
# ==========================================


# ------------------------------------------
# FICHIER : services\chunking\__init__.py
# ------------------------------------------


# ------------------------------------------
# FICHIER : services\chunking\enrichment.py
# ------------------------------------------
import logging
from repositories.prompt import prompt_repository
from services.llm import llm_service
from utils.json_parser import robust_json_parse

logger = logging.getLogger(__name__)

class EnrichmentService:
    
    def extract_metadata(self, text: str, prompt_name: str) -> dict:
        """
        Pour les prompts g√©n√©riques (Profil, Post, Synth√®se...) qui agissent comme une instruction Syst√®me.
        """
        prompt_doc = prompt_repository.get_by_name(prompt_name)
        if not prompt_doc:
            logger.warning(f"‚ö†Ô∏è Prompt '{prompt_name}' introuvable.")
            return {}

        if not text or len(text) < 10:
            return {}

        logger.info(f"üß† [Enrichment] JSON Prompt '{prompt_name}'...")
        
        # Cas standard : Le prompt en base est le System Prompt
        raw_response = llm_service.generate_response(
            system_prompt=prompt_doc.prompt,
            user_input=f"Analyse ce texte :\n\n{text}",
        )

        return robust_json_parse(raw_response) or {}

    def generate_hypothetical_questions(self, text: str) -> str:
        """ Reverse HyDE (Questions) """
        prompt_name = "chunk_hypothetical_questions"
        prompt_doc = prompt_repository.get_by_name(prompt_name)
        
        if not prompt_doc:
            return ""

        formatted_prompt = prompt_doc.prompt.replace("${text}", text) # Syntaxe JS du prompt

        response = llm_service.generate_response(
            system_prompt="Tu es un expert en g√©n√©ration de questions.",
            user_input=formatted_prompt
        )
        return response.strip()

    def extract_entities(self, text: str) -> dict:
        """
        Sp√©cifique pour 'chunk_entities' qui extrait People, Companies, Tools...
        """
        prompt_name = "chunk_entities"
        prompt_doc = prompt_repository.get_by_name(prompt_name)
        
        if not prompt_doc:
            logger.warning(f"‚ö†Ô∏è Prompt '{prompt_name}' introuvable.")
            return {}

        if len(text) < 50:
            return {}

        logger.info(f"üß† [Enrichment] Extracting Entities (~{len(text)} chars)...")

        # Le prompt contient "{text}", on le remplace
        # Note : On utilise .replace() simple pour √™tre s√ªr
        formatted_prompt = prompt_doc.prompt.replace("{text}", text)

        raw_response = llm_service.generate_response(
            system_prompt="Tu es un expert en extraction d'entit√©s nomm√©es (NER).",
            user_input=formatted_prompt
        )
        
        return robust_json_parse(raw_response) or {}

enrichment_service = EnrichmentService()

# ------------------------------------------
# FICHIER : services\chunking\factory.py
# ------------------------------------------
from typing import Dict, Any, List
from schemas.doc import Chunk as ChunkSchema

class ChunkFactory:
    """
    Centralise la cr√©ation des objets Chunk pour garantir un format de m√©tadonn√©es coh√©rent.
    """

    @staticmethod
    def create_chunk(content: str, doc_id: str, type_chunk: str, extra_meta: Dict[str, Any] = {}) -> ChunkSchema:
        """Helper g√©n√©rique"""
        # On fusionne les m√©tadonn√©es de base avec les sp√©cifiques
        metadata = {
            "doc": doc_id,
            "type": type_chunk,
            **extra_meta
        }
        
        # Nettoyage des valeurs None pour Chroma
        clean_meta = {k: v for k, v in metadata.items() if v is not None}

        return ChunkSchema(
            content=content,
            metadata=clean_meta
        )

    @staticmethod
    def create_experience_chunk(doc_id: str, exp: Dict[str, Any]) -> ChunkSchema:
        """Cr√©e un chunk sp√©cifique pour une exp√©rience professionnelle"""
        title = exp.get("title", "Poste inconnu")
        company = exp.get("company", "Entreprise inconnue")
        description = exp.get("description", "")
        dates = exp.get("date_range", "")

        content = f"Poste : {title}\nEntreprise : {company}\nP√©riode : {dates}\nDescription : {description}"
        
        # Texte optimis√© pour la recherche vectorielle (Embedding)
        # Ici on garde le m√™me contenu, mais on pourrait le r√©sumer
        
        return ChunkFactory.create_chunk(
            content=content,
            doc_id=doc_id,
            type_chunk="experience",
            extra_meta={
                "company": company,
                "title": title,
                "source_type": "profile_structure"
            }
        )

    @staticmethod
    def create_education_chunk(doc_id: str, edu: Dict[str, Any]) -> ChunkSchema:
        school = edu.get("school", "√âcole inconnue")
        degree = edu.get("degree", "")
        
        content = f"√âcole : {school}\nDipl√¥me : {degree}"
        
        return ChunkFactory.create_chunk(
            content=content,
            doc_id=doc_id,
            type_chunk="education",
            extra_meta={
                "school": school,
                "degree": degree,
                "source_type": "profile_structure"
            }
        )

chunk_factory = ChunkFactory()

# ------------------------------------------
# FICHIER : services\chunking\manager.py
# ------------------------------------------
import logging
import uuid
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter # pip install langchain-text-splitters
from schemas.doc import Chunk as ChunkSchema
from services.llm import llm_service

logger = logging.getLogger(__name__)

class ChunkingManager:
    
    def __init__(self):
        # Configuration standard RAG
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", " ", ""]
        )

    def chunk_data(self, doc_id: str, text_content: str, category: str, tags: List[str]) -> List[ChunkSchema]:
        """
        D√©coupe le texte, g√©n√®re les embeddings et formate pour ChromaDB.
        """
        logger.info(f"‚úÇÔ∏è Chunking started for {doc_id} ({len(text_content)} chars)")
        
        # 1. D√©coupage
        raw_chunks = self.splitter.create_documents([text_content])
        chunk_schemas = []

        # 2. Traitement & Embeddings
        for i, doc in enumerate(raw_chunks):
            content = doc.page_content
            
            # G√©n√©ration embedding (appel LLM ou mod√®le local)
            # Note: Dans le code Node, c'√©tait fait via `llmRepository.callLlmEmbeddings`
            # Ici on simule ou on appelle votre service existant s'il supporte l'embedding
            # embeddings = llm_service.generate_embeddings(content) 
            embeddings = [] # Placeholder si pas de mod√®le d'embedding configur√© dans llm.py

            metadata = {
                "doc": doc_id,
                "chunk_index": i,
                "category": category,
                "tags": ",".join(tags),
                "source_type": "file_upload",
                "length": len(content)
            }

            chunk_schemas.append(ChunkSchema(
                content=content,
                metadata=metadata,
                embeddings=embeddings
            ))

        logger.info(f"‚úÖ {len(chunk_schemas)} chunks g√©n√©r√©s pour {doc_id}")
        return chunk_schemas

chunking_manager = ChunkingManager()

# ------------------------------------------
# FICHIER : services\chunking\strategies\base.py
# ------------------------------------------
from abc import ABC, abstractmethod
from typing import List, Any
from schemas.doc import Chunk as ChunkSchema # <-- Import essentiel ici

class ChunkingStrategy(ABC):
    @abstractmethod
    def execute(self, doc_id: str, data: Any, tags: List[str]) -> List[ChunkSchema]:
        pass

# ------------------------------------------
# FICHIER : services\chunking\strategies\general.py
# ------------------------------------------
from typing import List, Any
from langchain_text_splitters import RecursiveCharacterTextSplitter

from schemas.doc import Chunk as ChunkSchema
from ..factory import chunk_factory
from .base import ChunkingStrategy
from services.chunking.enrichment import enrichment_service 

class GeneralStrategy(ChunkingStrategy):
    def __init__(self):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )

    def execute(self, doc_id: str, data: Any, tags: List[str]) -> List[ChunkSchema]:
        text_content = ""
        if isinstance(data, str):
            text_content = data
        elif isinstance(data, dict):
            text_content = data.get("text_content") or data.get("body") or data.get("content") or str(data)
        
        if not text_content:
            return []

        # 1. D√©coupage
        raw_chunks = self.splitter.create_documents([text_content])
        
        final_chunks = []
        for i, rc in enumerate(raw_chunks):
            content_for_chunk = rc.page_content
            
            # 2. ENRICHISSEMENT 1 : Questions Hypoth√©tiques
            if len(rc.page_content) > 500:
              questions = enrichment_service.generate_hypothetical_questions(rc.page_content)
            if questions:
                content_for_chunk += f"\n\n--- Questions Potentielles ---\n{questions}"

            # 3. ENRICHISSEMENT 2 : Entit√©s Nomm√©es (chunk_entities)
            entities = enrichment_service.extract_entities(rc.page_content)
            
            # Pr√©paration des m√©tadonn√©es aplaties pour Chroma
            extra_meta = {
                "index": i,
                "tags": ",".join(tags), # Toujours aplatir les listes
                "has_questions": bool(questions)
            }

            # On injecte les entit√©s si trouv√©es
            # Le prompt renvoie : { "people": [{"name": "..."}], "companies": ... }
            if entities:
                if "people" in entities and entities["people"]:
                    names = [p["name"] for p in entities["people"] if "name" in p]
                    extra_meta["entities_people"] = ", ".join(names)
                
                if "companies" in entities and entities["companies"]:
                    names = [c["name"] for c in entities["companies"] if "name" in c]
                    extra_meta["entities_companies"] = ", ".join(names)
                
                if "tools" in entities and entities["tools"]:
                    names = [t["name"] for t in entities["tools"] if "name" in t]
                    extra_meta["entities_tools"] = ", ".join(names)

                if "locations" in entities and entities["locations"]:
                    names = [l["name"] for l in entities["locations"] if "name" in l]
                    extra_meta["entities_locations"] = ", ".join(names)

            chunk = chunk_factory.create_chunk(
                content=content_for_chunk,
                doc_id=doc_id,
                type_chunk="general",
                extra_meta=extra_meta
            )
            final_chunks.append(chunk)
            
        return final_chunks

# ------------------------------------------
# FICHIER : services\chunking\strategies\post.py
# ------------------------------------------
from typing import List, Any
from schemas.doc import Chunk as ChunkSchema
from ..factory import chunk_factory
from .base import ChunkingStrategy
from services.chunking.enrichment import enrichment_service

class PostStrategy(ChunkingStrategy):
    def execute(self, doc_id: str, data: Any, tags: List[str]) -> List[ChunkSchema]:
        """
        Traite un Post LinkedIn/Social.
        Utilise le prompt 'chunk_post' pour extraire les m√©tadonn√©es avant de chunker.
        """
        # 1. R√©cup√©ration du texte brut
        text_content = ""
        if isinstance(data, str):
            text_content = data
        elif isinstance(data, dict):
            text_content = data.get("post_text") or data.get("text") or str(data)

        if not text_content:
            return []

        # 2. Enrichissement via LLM (Prompt: chunk_post)
        # Ce prompt extrait : name, company, industry, dates, likes_count, type...
        meta_extracted = enrichment_service.extract_metadata(text_content, "chunk_post")
        
        # 3. Cr√©ation du Chunk unique (un post est rarement assez long pour √™tre d√©coup√©)
        # On injecte les m√©tadonn√©es extraites pour le filtrage futur
        
        chunk = chunk_factory.create_chunk(
            content=text_content,
            doc_id=doc_id,
            type_chunk="post",
            extra_meta={
                "tags": ",".join(tags),
                **meta_extracted  # Fusionne les infos extraites (ex: author, date...)
            }
        )

        return [chunk]

# ------------------------------------------
# FICHIER : services\chunking\strategies\profile.py
# ------------------------------------------
from typing import List, Any
from schemas.doc import Chunk as ChunkSchema
from ..factory import chunk_factory
from .base import ChunkingStrategy
from .general import GeneralStrategy

# IMPORT NOUVEAU
from services.chunking.enrichment import enrichment_service 

class ProfileStrategy(ChunkingStrategy):
    def __init__(self):
        self.text_strategy = GeneralStrategy()

    def execute(self, doc_id: str, data: Any, tags: List[str]) -> List[ChunkSchema]:
        if not isinstance(data, dict):
            return self.text_strategy.execute(doc_id, data, tags)

        chunks = []
        person_name = data.get("name", "Inconnu")

        # 1. Chunk "About" + ENRICHISSEMENT
        if data.get("about"):
            about_text = f"√Ä propos de {person_name} : {data['about']}"
            
            # --- NOUVEAU : Appel LLM pour extraire les skills ---
            # On utilise le prompt 'chunk_about' que vous avez migr√© en base
            enriched_meta = enrichment_service.extract_metadata(data['about'], "chunk_about")
            # ----------------------------------------------------

            about_chunks = self.text_strategy.execute(doc_id, about_text, tags)
            
            for c in about_chunks:
                c.metadata["type"] = "profile_about"
                c.metadata["person_name"] = person_name
                
                # Injection des m√©tadonn√©es LLM (Skills, etc.)
                if enriched_meta:
                    # On convertit les listes en strings pour Chroma
                    if "hard_skills" in enriched_meta:
                        c.metadata["hard_skills"] = ", ".join(enriched_meta["hard_skills"])
                    if "soft_skills" in enriched_meta:
                        c.metadata["soft_skills"] = ", ".join(enriched_meta["soft_skills"])
                    if "languages" in enriched_meta:
                        c.metadata["languages"] = ", ".join(enriched_meta["languages"])

            chunks.extend(about_chunks)

        # 2. Chunks "Experience"
        experiences = data.get("experience", [])
        if isinstance(experiences, list):
            for exp in experiences:
                chunk = chunk_factory.create_experience_chunk(doc_id, exp, person_name)
                chunk.metadata["tags"] = ",".join(tags)
                chunks.append(chunk)

        # 3. Chunks "Education"
        education = data.get("education", [])
        if isinstance(education, list):
            for edu in education:
                chunk = chunk_factory.create_education_chunk(doc_id, edu, person_name)
                chunk.metadata["tags"] = ",".join(tags)
                chunks.append(chunk)

        return chunks
