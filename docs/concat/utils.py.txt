# ==========================================
# GROUPE : utils
# ==========================================


# ------------------------------------------
# FICHIER : utils\json_parser.py
# ------------------------------------------
import json
import logging

logger = logging.getLogger(__name__)

def robust_json_parse(llm_output: str) -> dict | list | None:
    """
    Extrait un JSON (Objet ou Liste) de manière robuste sans Regex complexes.
    """
    if not llm_output:
        return None

    text = llm_output.strip()

    # 1. Nettoyage basique des balises Markdown ```json ... ```
    if "```" in text:
        lines = text.splitlines()
        # On filtre les lignes qui contiennent ```
        text = "\n".join([l for l in lines if "```" not in l]).strip()

    # 2. Recherche des bornes JSON (Optimisé)
    # On cherche le premier '[' ou '{'
    first_square = text.find('[')
    first_curly = text.find('{')
    
    start_index = -1
    is_array = False

    # Détermination du début (lequel arrive en premier ?)
    if first_square != -1 and first_curly != -1:
        if first_square < first_curly:
            start_index = first_square
            is_array = True
        else:
            start_index = first_curly
    elif first_square != -1:
        start_index = first_square
        is_array = True
    elif first_curly != -1:
        start_index = first_curly
    
    if start_index == -1:
        return None

    # Détermination de la fin correspondante
    end_char = ']' if is_array else '}'
    end_index = text.rfind(end_char)

    if end_index == -1 or end_index < start_index:
        return None

    json_candidate = text[start_index : end_index + 1]

    try:
        return json.loads(json_candidate)
    except json.JSONDecodeError as e:
        logger.error(f"JSON Decode Error: {e}")
        return None

# ------------------------------------------
# FICHIER : utils\linkedin_cleaner.py
# ------------------------------------------
# utils/linkedin_cleaner.py
import re
from urllib.parse import urlparse, urlunparse

def clean_linkedin_url(url: str) -> str:
    """
    Nettoie une URL LinkedIn pour ne garder que la partie canonique.
    Ex: https://www.linkedin.com/in/jean-dupont?src=... -> https://www.linkedin.com/in/jean-dupont
    """
    if not url:
        return ""
        
    try:
        # 1. Parsing basique
        parsed = urlparse(url.strip())
        
        # 2. On reconstruit sans les query params (partie après ?) ni fragments (partie après #)
        # On garde scheme (https), netloc (www.linkedin.com) et path (/in/jean-dupont)
        cleaned = urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
        
        # 3. Retirer le slash final s'il existe (convention souvent utilisée pour les IDs)
        if cleaned.endswith("/"):
            cleaned = cleaned[:-1]
            
        return cleaned
        
    except Exception:
        # Fallback si l'URL est malformée, on renvoie telle quelle
        return url

# ------------------------------------------
# FICHIER : utils\text_extractor.py
# ------------------------------------------
import io
import logging
import pypdf # Nécessite pip install pypdf

logger = logging.getLogger(__name__)

class TextExtractor:
    
    def extract_from_file(self, file_path: str, content_type: str = "text/plain") -> str:
        try:
            if content_type == "application/pdf" or file_path.endswith(".pdf"):
                return self._extract_pdf(file_path)
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.error(f"❌ Erreur extraction texte : {e}")
            return ""

    def _extract_pdf(self, file_path: str) -> str:
        text = ""
        try:
            with open(file_path, 'rb') as f:
                reader = pypdf.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text() + "\n\n"
            return text
        except Exception as e:
            logger.error(f"❌ Erreur lecture PDF : {e}")
            raise e

    def extract_from_bytes(self, file_bytes: bytes, filename: str) -> str:
        """Pour traitement direct depuis la RAM (UploadFile)"""
        try:
            if filename.lower().endswith(".pdf"):
                reader = pypdf.PdfReader(io.BytesIO(file_bytes))
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n\n"
                return text
            else:
                return file_bytes.decode('utf-8')
        except Exception as e:
            logger.error(f"❌ Erreur extraction bytes : {e}")
            raise e

text_extractor = TextExtractor()
