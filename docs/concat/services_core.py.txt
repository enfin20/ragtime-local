# ==========================================
# GROUPE : services_core
# ==========================================


# ------------------------------------------
# FICHIER : services\__init__.py
# ------------------------------------------


# ------------------------------------------
# FICHIER : services\agent_tools.py
# ------------------------------------------
import logging
import json
from typing import List, Dict, Any
from repositories.chunk import chunk_repository
from repositories.prompt import prompt_repository
from services.llm import llm_service
from utils.json_parser import robust_json_parse

logger = logging.getLogger(__name__)

class AgentToolExecutor:
    
    def __init__(self, employee: str):
        self.employee = employee

    def exploratory_search(self, query: str, tags: List[str], exclude: dict) -> List[dict]:
        """
        R√©cup√®re un large set de candidats (50) via recherche vectorielle pure.
        """
        logger.info(f"üîç [Tool] Exploratory Search: '{query}'")
        
        # 1. R√©cup√©ration des IDs valides
        from repositories.doc import doc_repository
        doc_ids = doc_repository.get_filtered_doc_ids(self.employee, tags)
        
        if not doc_ids:
            logger.warning("   ‚ö†Ô∏è Scope vide (aucun doc correspondant aux tags).")
            return []

        # 2. Recherche large (Limit 50)
        results = chunk_repository.search(
            query=query,
            employee=self.employee,
            limit=50, 
            doc_ids_filter=doc_ids
        )

        candidates = []
        if results['documents']:
            for i, content in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                
                # Exclusion basique
                if exclude.get('sources') and meta.get('source') in exclude['sources']: continue
                if exclude.get('origins') and meta.get('origin') in exclude['origins']: continue

                candidates.append({
                    "content": content,
                    "metadata": meta,
                    "id": results['ids'][0][i]
                })
        
        logger.info(f"   -> {len(candidates)} candidats vectoriels trouv√©s.")
        return candidates

    def rerank_chunks(self, question: str, chunks: List[dict]) -> List[dict]:
        """
        √âvalue la pertinence s√©mantique via LLM (Prompt: agent_rerank).
        """
        if not chunks:
            return []

        # --- S√âCURIT√â LLM LOCAL ---
        # On limite √† 5 ou 7 chunks max pour le reranking pour √©viter que Llama3 ne sature
        candidates = chunks[:7] 
        logger.info(f"‚öñÔ∏è [Tool] Reranking de {len(candidates)} chunks (sur {len(chunks)} re√ßus)...")
        
        prompt_doc = prompt_repository.get_by_name("agent_rerank")
        if not prompt_doc:
            logger.error("‚ùå Prompt 'agent_rerank' introuvable ! Fallback sur les 5 premiers.")
            return candidates[:5]

        # Pr√©paration du contexte
        context_text = ""
        for i, c in enumerate(candidates):
            preview = c['content'][:1200].replace("\n", " ") # Nettoyage l√©ger
            context_text += f"--- Chunk {i} ---\n{preview}\n\n"

        final_prompt = prompt_doc.prompt.replace("{question}", question).replace("{context}", context_text)

        try:
            # Appel LLM
            llm_response = llm_service.generate_response(
                system_prompt="Tu es un syst√®me de scoring JSON. R√©ponds UNIQUEMENT avec un tableau JSON.",
                user_input=final_prompt
            )
            
            # Parsing robuste (G√®re les tableaux [...])
            scores_data = robust_json_parse(llm_response)
            
            if isinstance(scores_data, dict): scores_data = [scores_data]
            
            if not isinstance(scores_data, list):
                logger.warning(f"‚ö†Ô∏è Format JSON invalide du Reranker. Fallback.")
                return candidates[:3]

            scored_chunks = []
            for item in scores_data:
                idx = item.get("chunk_index")
                score = item.get("score", 0.0)
                
                # Validation des index
                if idx is not None and isinstance(idx, int) and 0 <= idx < len(candidates):
                    chunk = candidates[idx]
                    chunk["score"] = score
                    
                    # Seuil de pertinence (0.5 = Moyen/Bon)
                    if score >= 0.5:
                        scored_chunks.append(chunk)

            # Tri par pertinence
            scored_chunks.sort(key=lambda x: x["score"], reverse=True)
            
            logger.info(f"   ‚úÖ Reranking termin√©. {len(scored_chunks)} chunks retenus.")
            
            # Fallback si tout est rejet√© (pour √©viter une r√©ponse vide)
            if not scored_chunks and candidates:
                logger.warning("   ‚ö†Ô∏è Le Reranker a tout rejet√©. R√©cup√©ration de s√©curit√© du Top 1.")
                candidates[0]["score"] = 0.4
                return [candidates[0]]

            return scored_chunks

        except Exception as e:
            logger.error(f"‚ùå Erreur critique durant le Reranking: {e}")
            return candidates[:3] # Fallback de s√©curit√©

# ------------------------------------------
# FICHIER : services\chat.py
# ------------------------------------------
import logging
from repositories.prompt import prompt_repository
from services.llm import llm_service
from schemas.chat import ChatRequestNode
from services.agent_tools import AgentToolExecutor

logger = logging.getLogger(__name__)

class ChatService:
    
    def handle_node_chat(self, request: ChatRequestNode) -> dict:
        logger.info(f"üöÄ [CHAT] Demande re√ßue: Prompt='{request.prompt}' | Q='{request.question}'")

        # 1. Initialisation des outils
        tools = AgentToolExecutor(request.employee)
        
        # 2. Gestion Exclusion (Standard)
        exclude = request.exclude if isinstance(request.exclude, dict) else {}
        if "archive" not in exclude.get("categories", []):
            exclude.setdefault("categories", []).append("archive")

        # ==========================================================
        # BRANCHE 1 : WORKFLOW EXPERT (Prompt d√©fini)
        # Logique: Search -> Rerank -> Inject -> Execute Prompt
        # ==========================================================
        if request.prompt:
            return self._handle_expert_workflow(request, tools, exclude)

        # ==========================================================
        # BRANCHE 2 : WORKFLOW STANDARD (Chat Agentique)
        # ==========================================================
        return self._handle_standard_workflow(request, tools, exclude)

    def _handle_expert_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        logger.info("üß† [Workflow] D√©marrage mode EXPERT (Data-First)")

        # A. R√©cup√©ration du Prompt Expert
        prompt_doc = prompt_repository.get_by_name(request.prompt)
        if not prompt_doc:
            return {"response": f"Erreur: Prompt '{request.prompt}' introuvable.", "sources": []}

        # B. D√©finition de la cible (Target Input)
        # Node: const targetInput = question ... ? question : "Analyse globale..."
        target_input = request.question if request.question and request.question.strip() else "Analyse globale du contenu principal"
        
        # C. √âTAPE 1 : Exploratory Search (Vectorielle pure)
        raw_candidates = tools.exploratory_search(target_input, request.tags, exclude)
        
        # D. √âTAPE 2 : Reranking Intelligent (LLM)
        # C'est ici que la magie op√®re : le LLM d√©cide si "Article IA" correspond √† "Analyse globale"
        relevant_chunks = tools.rerank_chunks(target_input, raw_candidates)

        # E. Construction du Contexte pour le Prompt
        context_str = ""
        sources_list = []
        
        if relevant_chunks:
            context_str = "\n\n".join([
                f"--- Document {i+1} (Source: {c['metadata'].get('doc')}) ---\n{c['content']}" 
                for i, c in enumerate(relevant_chunks)
            ])
            # Formatage des sources pour la r√©ponse API
            seen_docs = set()
            for c in relevant_chunks:
                doc_name = c['metadata'].get('doc', 'inconnu')
                if doc_name not in seen_docs:
                    sources_list.append({"name": doc_name, "score": c.get("score", 1.0)})
                    seen_docs.add(doc_name)
        else:
            logger.warning("‚ö†Ô∏è [Expert] Aucun chunk pertinent apr√®s Reranking.")

        # F. Construction du Message Final (Prompt Engineering)
        # Structure Node.js :
        # 1. System Prompt (Le prompt expert charg√©)
        # 2. User Message : "Voici les donn√©es... CONTEXTE... INSTRUCTION..."
        
        system_msg = prompt_doc.prompt
        
        user_msg_content = f"""
        Voici les donn√©es contextuelles r√©cup√©r√©es concernant "{target_input}".
        
        CONTEXTE:
        {context_str}
        
        INSTRUCTION:
        Ex√©cute maintenant ta mission d'expert en utilisant ces donn√©es.
        """
        
        if not relevant_chunks:
            user_msg_content = f"Aucune donn√©e sp√©cifique trouv√©e pour '{target_input}'. Fais de ton mieux."

        # G. Appel final LLM
        # On concat√®ne l'historique si pr√©sent
        history_block = ""
        if request.history:
            history_block = "HISTORIQUE DE CONVERSATION:\n" + "\n".join(
                [f"{m.get('role','').upper()}: {m.get('content','')}" for m in request.history]
            ) + "\n\n"

        # Note: On triche un peu en mettant tout dans le user_input pour simplifier l'appel √† llm_service
        # mais on respecte la structure logique.
        full_prompt = f"{system_msg}\n\n{history_block}\nUSER TASK:\n{user_msg_content}"

        response = llm_service.generate_response(
            system_prompt="Tu es un expert qualifi√©.",
            user_input=full_prompt
        )

        return {
            "response": response,
            "sources": sources_list
        }

    def _handle_standard_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        # Impl√©mentation simplifi√©e du chat standard utilisant aussi le reranking
        query = request.question if request.question.strip() else "R√©sum√©"
        
        raw = tools.exploratory_search(query, request.tags, exclude)
        refined = tools.rerank_chunks(query, raw)
        
        context_str = "\n".join([c['content'] for c in refined])
        
        answer = llm_service.generate_response(
            system_prompt="Tu es un assistant utile. R√©ponds en fran√ßais.",
            user_input=query,
            context=context_str
        )
        
        sources = [{"name": c['metadata'].get('doc'), "score": c.get("score")} for c in refined]
        
        return {"response": answer, "sources": sources}

chat_service = ChatService()

# ------------------------------------------
# FICHIER : services\ingestion.py
# ------------------------------------------
import os
import logging
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

from schemas.doc import DocCreate
from repositories.doc import doc_repository
from repositories.chunk import chunk_repository
from repositories.tavily import tavily_repository
from utils.text_extractor import text_extractor
from services.chunking.manager import chunking_manager
from services.chunking.enrichment import enrichment_service

logger = logging.getLogger(__name__)

class IngestionService:
    
    def _fallback_scrape(self, url: str) -> str:
        """
        Plan B : Scraping classique si l'API Tavily √©choue.
        """
        logger.info(f"üõ°Ô∏è [Fallback] Tentative de scraping standard pour : {url}")
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # On essaie de r√©cup√©rer le texte principal
            # On vire les scripts et styles
            for script in soup(["script", "style", "nav", "footer"]):
                script.decompose()
                
            text = soup.get_text(separator="\n")
            
            # Nettoyage basique des lignes vides
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            clean_text = '\n'.join(chunk for chunk in chunks if chunk)
            
            if len(clean_text) < 50:
                return ""
                
            return clean_text
            
        except Exception as e:
            logger.error(f"‚ùå [Fallback] Echec aussi : {e}")
            return ""

    def process_input(self, input_data: str | dict, employee: str, tags: list[str], origin: str = "manual"):
        doc_name = "Document sans titre"
        content_data = "" 
        source_type = "manual"
        category = "document"

        if isinstance(input_data, str) and (input_data.startswith("http://") or input_data.startswith("https://")):
            logger.info(f"üîó URL d√©tect√©e : {input_data}")
            source_type = "web"
            category = "website"
            doc_name = input_data
            
            # [cite_start]1. Tentative Tavily [cite: 117]
            content_data = tavily_repository.extract_content(input_data)
            
            # 2. Tentative Fallback si vide
            if not content_data:
                logger.warning(f"‚ö†Ô∏è Tavily n'a rien renvoy√© pour {input_data}. Passage au Fallback.")
                content_data = self._fallback_scrape(input_data)

            if not content_data:
                raise Exception(f"Contenu vide pour l'URL (Protection anti-bot ou erreur) : {input_data}")

        elif isinstance(input_data, str) and os.path.exists(input_data) and os.path.isfile(input_data):
            logger.info(f"üìÅ Fichier local d√©tect√© : {input_data}")
            source_type = "file"
            doc_name = os.path.basename(input_data)
            
            if input_data.endswith(".json"):
                with open(input_data, 'r', encoding='utf-8') as f:
                    content_data = json.load(f)
                category = "profile"
            else:
                content_data = text_extractor.extract_from_file(input_data)
                category = "document"

        else:
            logger.info("üì¶ Donn√©e brute d√©tect√©e")
            source_type = "raw"
            
            if isinstance(input_data, dict):
                content_data = input_data
                if "post_text" in input_data or "text" in input_data:
                    category = "post"
                    doc_name = f"post_{int(datetime.now().timestamp())}"
                else:
                    category = "profile"
                    doc_name = f"profile_{input_data.get('name', 'unknown')}"
            else:
                doc_name = f"text_{int(datetime.now().timestamp())}.txt"
                content_data = input_data

        return self._process_content(doc_name, content_data, category, employee, tags, origin, source_type)

    def _process_content(self, doc_id: str, content_data: any, category: str, employee: str, tags: list[str], origin: str, source: str):
        job_id = f"job_{int(datetime.now().timestamp())}"
        
        preview_text = str(content_data)[:3000]
        synthesis_data = {}
        
        try:
            logger.info(f"üß† [Ingestion] Generating synthesis for {doc_id}...")
            synthesis_input = f"Doc: {doc_id}\nContent: {preview_text}"
            synthesis_data = enrichment_service.extract_metadata(synthesis_input, "synthesis_tags")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to generate synthesis: {e}")

        doc_synthesis = synthesis_data.get("synthesis", "")
        doc_suggested_tags = synthesis_data.get("suggested_tags", [])

        page_content_storage = content_data if isinstance(content_data, dict) else {"text": content_data}

        doc_data = DocCreate(
            doc=doc_id,
            category=category,
            source=source,
            origin=origin,
            tags=tags,
            status="Processing",
            employee=employee,
            job_id=job_id,
            page_content=page_content_storage,
            synthesis=doc_synthesis,
            suggested_tags=doc_suggested_tags
        )
        doc_repository.upsert_doc(doc_data)

        try:
            chunks_to_save = chunking_manager.chunk_data(doc_id, content_data, category, tags)

            if not chunks_to_save:
                logger.warning(f"‚ö†Ô∏è Aucun chunk g√©n√©r√© pour {doc_id}")

            chunk_repository.add_chunks(doc_id, employee, chunks_to_save)

            doc_repository.update_status(doc_id, employee, "Done")
            logger.info(f"‚úÖ [Ingestion] Success. {len(chunks_to_save)} chunks created via strategy.")
            
            return {
                "status": "success", 
                "doc_id": doc_id,
                "chunks_count": len(chunks_to_save),
                "strategy": category
            }

        except Exception as e:
            logger.error(f"‚ùå [Ingestion] Error: {e}")
            doc_repository.update_status(doc_id, employee, "Failed")
            raise e



ingestion_service = IngestionService()

# ------------------------------------------
# FICHIER : services\llm.py
# ------------------------------------------
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class LLMService:
    def __init__(self, model_name: str = "llama3.2"):
        self.model_name = model_name
        self.llm = ChatOllama(model=model_name, temperature=0)

    def generate_response(self, system_prompt: str, user_input: str, context: str = "") -> str:
        """
        G√©n√®re une r√©ponse simple bas√©e sur un prompt syst√®me et une entr√©e utilisateur.
        """
        prompt = ChatPromptTemplate.from_template("""
        {system_instruction}
        
        CONTEXTE (Si fourni) :
        {context}
        
        QUESTION : {input}
        """)

        chain = prompt | self.llm | StrOutputParser()
        
        return chain.invoke({
            "system_instruction": system_prompt,
            "context": context,
            "input": user_input
        })

# Instance par d√©faut (facile √† importer)
llm_service = LLMService()
