# ==========================================
# GROUPE : services_core
# ==========================================


# ------------------------------------------
# FICHIER : services\__init__.py
# ------------------------------------------


# ------------------------------------------
# FICHIER : services\agent_tools.py
# ------------------------------------------
import logging
import json
from typing import List, Dict, Any
from repositories.chunk import chunk_repository
from repositories.prompt import prompt_repository
from services.llm import llm_service
from utils.json_parser import robust_json_parse

logger = logging.getLogger(__name__)

class AgentToolExecutor:
    
    def __init__(self, employee: str):
        self.employee = employee

    def exploratory_search(self, query: str, tags: List[str], exclude: dict) -> List[dict]:
        """
        R√©cup√®re un large set de candidats (50) via recherche vectorielle pure.
        """
        logger.info(f"üîç [Tool] Exploratory Search: '{query}'")
        
        # 1. R√©cup√©ration des IDs valides
        from repositories.doc import doc_repository
        doc_ids = doc_repository.get_filtered_doc_ids(self.employee, tags)
        
        if not doc_ids:
            logger.warning("   ‚ö†Ô∏è Scope vide (aucun doc correspondant aux tags).")
            return []

        # 2. Recherche large (Limit 50)
        results = chunk_repository.search(
            query=query,
            employee=self.employee,
            limit=50, 
            doc_ids_filter=doc_ids
        )

        candidates = []
        if results['documents']:
            for i, content in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                
                # Exclusion basique
                if exclude.get('sources') and meta.get('source') in exclude['sources']: continue
                if exclude.get('origins') and meta.get('origin') in exclude['origins']: continue

                candidates.append({
                    "content": content,
                    "metadata": meta,
                    "id": results['ids'][0][i]
                })
        
        logger.info(f"   -> {len(candidates)} candidats vectoriels trouv√©s.")
        return candidates

    def rerank_chunks(self, question: str, chunks: List[dict]) -> List[dict]:
        """
        √âvalue la pertinence s√©mantique via LLM (Prompt: agent_rerank).
        """
        if not chunks:
            return []

        # --- S√âCURIT√â LLM LOCAL ---
        # On limite √† 5 ou 7 chunks max pour le reranking pour √©viter que Llama3 ne sature
        candidates = chunks[:7] 
        logger.info(f"‚öñÔ∏è [Tool] Reranking de {len(candidates)} chunks (sur {len(chunks)} re√ßus)...")
        
        prompt_doc = prompt_repository.get_by_name("agent_rerank")
        if not prompt_doc:
            logger.error("‚ùå Prompt 'agent_rerank' introuvable ! Fallback sur les 5 premiers.")
            return candidates[:5]

        # Pr√©paration du contexte
        context_text = ""
        for i, c in enumerate(candidates):
            preview = c['content'][:1200].replace("\n", " ") # Nettoyage l√©ger
            context_text += f"--- Chunk {i} ---\n{preview}\n\n"

        final_prompt = prompt_doc.prompt.replace("{question}", question).replace("{context}", context_text)

        try:
            # Appel LLM
            llm_response = llm_service.generate_response(
                system_prompt="Tu es un syst√®me de scoring JSON. R√©ponds UNIQUEMENT avec un tableau JSON.",
                user_input=final_prompt
            )
            
            # Parsing robuste (G√®re les tableaux [...])
            scores_data = robust_json_parse(llm_response)
            
            if isinstance(scores_data, dict): scores_data = [scores_data]
            
            if not isinstance(scores_data, list):
                logger.warning(f"‚ö†Ô∏è Format JSON invalide du Reranker. Fallback.")
                return candidates[:3]

            scored_chunks = []
            for item in scores_data:
                idx = item.get("chunk_index")
                score = item.get("score", 0.0)
                
                # Validation des index
                if idx is not None and isinstance(idx, int) and 0 <= idx < len(candidates):
                    chunk = candidates[idx]
                    chunk["score"] = score
                    
                    # Seuil de pertinence (0.5 = Moyen/Bon)
                    if score >= 0.5:
                        scored_chunks.append(chunk)

            # Tri par pertinence
            scored_chunks.sort(key=lambda x: x["score"], reverse=True)
            
            logger.info(f"   ‚úÖ Reranking termin√©. {len(scored_chunks)} chunks retenus.")
            
            # Fallback si tout est rejet√© (pour √©viter une r√©ponse vide)
            if not scored_chunks and candidates:
                logger.warning("   ‚ö†Ô∏è Le Reranker a tout rejet√©. R√©cup√©ration de s√©curit√© du Top 1.")
                candidates[0]["score"] = 0.4
                return [candidates[0]]

            return scored_chunks

        except Exception as e:
            logger.error(f"‚ùå Erreur critique durant le Reranking: {e}")
            return candidates[:3] # Fallback de s√©curit√©

# ------------------------------------------
# FICHIER : services\chat.py
# ------------------------------------------
import logging
from repositories.prompt import prompt_repository
from services.llm import llm_service
from schemas.chat import ChatRequestNode
from services.agent_tools import AgentToolExecutor

logger = logging.getLogger(__name__)

class ChatService:
    
    def handle_node_chat(self, request: ChatRequestNode) -> dict:
        logger.info(f"üöÄ [CHAT] Demande re√ßue: Prompt='{request.prompt}' | Q='{request.question}'")

        # 1. Initialisation des outils
        tools = AgentToolExecutor(request.employee)
        
        # 2. Gestion Exclusion (Standard)
        exclude = request.exclude if isinstance(request.exclude, dict) else {}
        if "archive" not in exclude.get("categories", []):
            exclude.setdefault("categories", []).append("archive")

        # ==========================================================
        # BRANCHE 1 : WORKFLOW EXPERT (Prompt d√©fini)
        # Logique: Search -> Rerank -> Inject -> Execute Prompt
        # ==========================================================
        if request.prompt:
            return self._handle_expert_workflow(request, tools, exclude)

        # ==========================================================
        # BRANCHE 2 : WORKFLOW STANDARD (Chat Agentique)
        # ==========================================================
        return self._handle_standard_workflow(request, tools, exclude)

    def _handle_expert_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        logger.info("üß† [Workflow] D√©marrage mode EXPERT (Data-First)")

        # A. R√©cup√©ration du Prompt Expert
        prompt_doc = prompt_repository.get_by_name(request.prompt)
        if not prompt_doc:
            return {"response": f"Erreur: Prompt '{request.prompt}' introuvable.", "sources": []}

        # B. D√©finition de la cible (Target Input)
        # Node: const targetInput = question ... ? question : "Analyse globale..."
        target_input = request.question if request.question and request.question.strip() else "Analyse globale du contenu principal"
        
        # C. √âTAPE 1 : Exploratory Search (Vectorielle pure)
        raw_candidates = tools.exploratory_search(target_input, request.tags, exclude)
        
        # D. √âTAPE 2 : Reranking Intelligent (LLM)
        # C'est ici que la magie op√®re : le LLM d√©cide si "Article IA" correspond √† "Analyse globale"
        relevant_chunks = tools.rerank_chunks(target_input, raw_candidates)

        # E. Construction du Contexte pour le Prompt
        context_str = ""
        sources_list = []
        
        if relevant_chunks:
            context_str = "\n\n".join([
                f"--- Document {i+1} (Source: {c['metadata'].get('doc')}) ---\n{c['content']}" 
                for i, c in enumerate(relevant_chunks)
            ])
            # Formatage des sources pour la r√©ponse API
            seen_docs = set()
            for c in relevant_chunks:
                doc_name = c['metadata'].get('doc', 'inconnu')
                if doc_name not in seen_docs:
                    sources_list.append({"name": doc_name, "score": c.get("score", 1.0)})
                    seen_docs.add(doc_name)
        else:
            logger.warning("‚ö†Ô∏è [Expert] Aucun chunk pertinent apr√®s Reranking.")

        # F. Construction du Message Final (Prompt Engineering)
        # Structure Node.js :
        # 1. System Prompt (Le prompt expert charg√©)
        # 2. User Message : "Voici les donn√©es... CONTEXTE... INSTRUCTION..."
        
        system_msg = prompt_doc.prompt
        
        user_msg_content = f"""
        Voici les donn√©es contextuelles r√©cup√©r√©es concernant "{target_input}".
        
        CONTEXTE:
        {context_str}
        
        INSTRUCTION:
        Ex√©cute maintenant ta mission d'expert en utilisant ces donn√©es.
        """
        
        if not relevant_chunks:
            user_msg_content = f"Aucune donn√©e sp√©cifique trouv√©e pour '{target_input}'. Fais de ton mieux."

        # G. Appel final LLM
        # On concat√®ne l'historique si pr√©sent
        history_block = ""
        if request.history:
            history_block = "HISTORIQUE DE CONVERSATION:\n" + "\n".join(
                [f"{m.get('role','').upper()}: {m.get('content','')}" for m in request.history]
            ) + "\n\n"

        # Note: On triche un peu en mettant tout dans le user_input pour simplifier l'appel √† llm_service
        # mais on respecte la structure logique.
        full_prompt = f"{system_msg}\n\n{history_block}\nUSER TASK:\n{user_msg_content}"

        response = llm_service.generate_response(
            system_prompt="Tu es un expert qualifi√©.",
            user_input=full_prompt
        )

        return {
            "response": response,
            "sources": sources_list
        }

    def _handle_standard_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        # Impl√©mentation simplifi√©e du chat standard utilisant aussi le reranking
        query = request.question if request.question.strip() else "R√©sum√©"
        
        raw = tools.exploratory_search(query, request.tags, exclude)
        refined = tools.rerank_chunks(query, raw)
        
        context_str = "\n".join([c['content'] for c in refined])
        
        answer = llm_service.generate_response(
            system_prompt="Tu es un assistant utile. R√©ponds en fran√ßais.",
            user_input=query,
            context=context_str
        )
        
        sources = [{"name": c['metadata'].get('doc'), "score": c.get("score")} for c in refined]
        
        return {"response": answer, "sources": sources}

chat_service = ChatService()

# ------------------------------------------
# FICHIER : services\files.py
# ------------------------------------------
import logging
import shutil
import os
from datetime import datetime
from fastapi import UploadFile, BackgroundTasks

from repositories.doc import doc_repository
from repositories.chunk import chunk_repository
from schemas.doc import DocCreate
from utils.text_extractor import text_extractor
from services.chunking.manager import chunking_manager
from services.ingestion import ingestion_service # Pour r√©utiliser logic synth√®se si besoin

logger = logging.getLogger(__name__)

class FilesService:

    async def handle_add_file_workflow(self, file: UploadFile, tags: list[str], employee: str, job_id: str, background_tasks: BackgroundTasks):
        doc_name = file.filename
        
        # 1. Cr√©ation du Placeholder (Statut: Processing)
        # Permet de rendre la main tout de suite au client
        placeholder = DocCreate(
            doc=doc_name,
            category="document", # Sera affin√©
            source="manual",
            origin="upload",
            tags=tags,
            status="Processing",
            employee=employee,
            job_id=job_id,
            page_content={"name": doc_name, "loading": True}
        )
        doc_repository.upsert_doc(placeholder)
        logger.info(f"‚è≥ [Job {job_id}] Placeholder cr√©√© pour {doc_name}")

        # 2. Lecture du contenu (En m√©moire pour passer √† la t√¢che de fond)
        content_bytes = await file.read()
        
        # 3. Lancement T√¢che de Fond (Fire and Forget)
        background_tasks.add_task(
            self._process_file_background,
            doc_name,
            content_bytes,
            tags,
            employee,
            job_id,
            file.content_type
        )

        return {"status": "success", "message": "PROCESSING_STARTED", "job_id": job_id}

    def _process_file_background(self, doc_name: str, content_bytes: bytes, tags: list[str], employee: str, job_id: str, content_type: str):
        try:
            logger.info(f"üöÄ [Job {job_id}] D√©marrage traitement background : {doc_name}")

            # A. Extraction Texte
            text_content = ""
            if "pdf" in content_type or doc_name.endswith(".pdf"):
                # Sauvegarde temporaire si pypdf le n√©cessite, ou flux bytes
                text_content = text_extractor.extract_from_bytes(content_bytes, doc_name)
            else:
                text_content = content_bytes.decode("utf-8")

            if not text_content:
                raise Exception("Contenu extrait vide")

            # B. Classification / Synth√®se (Simplifi√© par rapport au Node qui utilise un Agent)
            # On peut imaginer ici un appel LLM pour d√©terminer la cat√©gorie
            category = "document" 
            
            # C. Chunking & Vectorisation
            chunks = chunking_manager.chunk_data(doc_name, text_content, category, tags)

            # D. Persistance Vectorielle (Chroma)
            chunk_repository.add_chunks(doc_name, employee, chunks)

            # E. Mise √† jour Document (Statut: Done)
            final_doc = DocCreate(
                doc=doc_name,
                category=category,
                source="manual",
                origin="upload",
                tags=tags,
                status="Done",
                employee=employee,
                job_id=job_id,
                page_content={
                    "text": text_content, 
                    "preview": text_content[:500]
                },
                quality=10.0 # Arbitraire pour fichier manuel
            )
            doc_repository.upsert_doc(final_doc)
            
            logger.info(f"‚úÖ [Job {job_id}] Traitement termin√© avec succ√®s pour {doc_name}")

        except Exception as e:
            logger.error(f"‚ùå [Job {job_id}] Echec traitement {doc_name}: {str(e)}")
            doc_repository.update_status(doc_name, employee, "Failed")

files_service = FilesService()

# ------------------------------------------
# FICHIER : services\ingestion.py
# ------------------------------------------
import os
import logging
import json
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

from schemas.doc import DocCreate
from repositories.doc import doc_repository
from repositories.chunk import chunk_repository
from repositories.tavily import tavily_repository
from utils.text_extractor import text_extractor
from services.chunking.manager import chunking_manager
from services.chunking.enrichment import enrichment_service

logger = logging.getLogger(__name__)

class IngestionService:

    def _clean_text_content(self, html_content: str) -> str:
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, 'html.parser')

        # 1. Suppression balises techniques/navigation
        for tag in soup(["script", "style", "nav", "footer", "aside", "form", "iframe", "noscript", "header"]):
            tag.decompose()

        # 2. Suppression par patterns CSS (Sidebar, Related, Comments)
        noise_patterns = re.compile(r'(related|recommend|share|social|comment|sidebar|newsletter|author-bio|cookie|promo)', re.IGNORECASE)
        for tag in soup.find_all(attrs={"class": noise_patterns}):
            tag.decompose()
        for tag in soup.find_all(attrs={"id": noise_patterns}):
            tag.decompose()

        text = soup.get_text(separator="\n")

        # 3. La "Guillotine" : on coupe d√®s qu'on voit ces titres
        stop_phrases = [
            "Related Articles", "You might also like", "Recommended for you",
            "Share this article", "About the Author", "Written by",
            "Submit an Article", "Editors Pick", "Read Next"
        ]
        
        lines = []
        for line in text.splitlines():
            clean_line = line.strip()
            if not clean_line:
                continue
            
            # Si une ligne commence par une phrase d'arr√™t, on stoppe tout
            stop_hit = False
            for phrase in stop_phrases:
                if clean_line.lower() == phrase.lower() or clean_line.lower().startswith(phrase.lower() + ":"):
                    stop_hit = True
                    break
            
            if stop_hit:
                logger.info(f"‚úÇÔ∏è  Texte coup√© au marqueur : '{clean_line}'")
                break 
            
            lines.append(clean_line)

        return "\n".join(lines)

    def _fallback_scrape(self, url: str) -> str:
        logger.info(f"üõ°Ô∏è [Fallback] Scraping : {url}")
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0"}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return self._clean_text_content(response.text)
        except Exception as e:
            logger.error(f"‚ùå [Fallback] Erreur : {e}")
            return ""

    def process_input(self, input_data: str | dict, employee: str, tags: list[str], origin: str = "manual"):
        doc_name = "Document sans titre"
        content_data = "" 
        source_type = "manual"
        category = "document"

        if isinstance(input_data, str) and (input_data.startswith("http://") or input_data.startswith("https://")):
            logger.info(f"üîó URL d√©tect√©e : {input_data}")
            source_type = "web"
            category = "website"
            doc_name = input_data
            
            # Nettoyage syst√©matique m√™me sur Tavily
            raw_tavily = tavily_repository.extract_content(input_data)
            if raw_tavily:
                content_data = self._clean_text_content(raw_tavily)
            else:
                content_data = self._fallback_scrape(input_data)

            if not content_data:
                raise Exception(f"Contenu vide pour l'URL : {input_data}")

        elif isinstance(input_data, str) and os.path.exists(input_data) and os.path.isfile(input_data):
            logger.info(f"üìÅ Fichier local : {input_data}")
            source_type = "file"
            doc_name = os.path.basename(input_data)
            if input_data.endswith(".json"):
                with open(input_data, 'r', encoding='utf-8') as f:
                    content_data = json.load(f)
                category = "profile"
            else:
                content_data = text_extractor.extract_from_file(input_data)
                category = "document"

        else:
            logger.info("üì¶ Donn√©e brute")
            source_type = "raw"
            if isinstance(input_data, dict):
                content_data = input_data
                if "post_text" in input_data or "text" in input_data:
                    category = "post"
                    doc_name = f"post_{int(datetime.now().timestamp())}"
                else:
                    category = "profile"
                    doc_name = f"profile_{input_data.get('name', 'unknown')}"
            else:
                doc_name = f"text_{int(datetime.now().timestamp())}.txt"
                content_data = input_data

        return self._process_content(doc_name, content_data, category, employee, tags, origin, source_type)

    def _process_content(self, doc_id: str, content_data: any, category: str, employee: str, tags: list[str], origin: str, source: str):
        job_id = f"job_{int(datetime.now().timestamp())}"
        preview_text = str(content_data)[:3000]
        
        # Synth√®se IA
        synthesis_data = {}
        try:
            synthesis_input = f"Doc: {doc_id}\nContent: {preview_text}"
            synthesis_data = enrichment_service.extract_metadata(synthesis_input, "synthesis_tags")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur Synth√®se: {e}")

        doc_data = DocCreate(
            doc=doc_id, category=category, source=source, origin=origin, tags=tags,
            status="Processing", employee=employee, job_id=job_id,
            page_content=content_data if isinstance(content_data, dict) else {"text": content_data},
            synthesis=synthesis_data.get("synthesis", ""),
            suggested_tags=synthesis_data.get("suggested_tags", [])
        )
        doc_repository.upsert_doc(doc_data)

        try:
            chunks_to_save = chunking_manager.chunk_data(doc_id, content_data, category, tags)
            if not chunks_to_save:
                logger.warning(f"‚ö†Ô∏è 0 chunk pour {doc_id}")
            
            chunk_repository.add_chunks(doc_id, employee, chunks_to_save)
            doc_repository.update_status(doc_id, employee, "Done")
            
            return {"status": "success", "doc_id": doc_id, "chunks_count": len(chunks_to_save), "strategy": category}

        except Exception as e:
            logger.error(f"‚ùå Erreur Ingestion: {e}")
            doc_repository.update_status(doc_id, employee, "Failed")
            raise e

ingestion_service = IngestionService()

# ------------------------------------------
# FICHIER : services\llm.py
# ------------------------------------------
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class LLMService:
    def __init__(self, model_name: str = "llama3.2"):
        self.model_name = model_name
        self.llm = ChatOllama(model=model_name, temperature=0)

    def generate_response(self, system_prompt: str, user_input: str, context: str = "") -> str:
        """
        G√©n√®re une r√©ponse simple bas√©e sur un prompt syst√®me et une entr√©e utilisateur.
        """
        prompt = ChatPromptTemplate.from_template("""
        {system_instruction}
        
        CONTEXTE (Si fourni) :
        {context}
        
        QUESTION : {input}
        """)

        chain = prompt | self.llm | StrOutputParser()
        
        return chain.invoke({
            "system_instruction": system_prompt,
            "context": context,
            "input": user_input
        })

# Instance par d√©faut (facile √† importer)
llm_service = LLMService()
