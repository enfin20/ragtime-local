# ==========================================
# GROUPE : scripts
# ==========================================


# ------------------------------------------
# FICHIER : scripts\__init__.py
# ------------------------------------------


# ------------------------------------------
# FICHIER : scripts\clean_data.py
# ------------------------------------------
import sys
import os
from pathlib import Path
from sqlalchemy.orm import Session

# Path hack
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine, get_chroma_client
from database.models import DocModel, ApiLogModel

LOG_DIR = Path("logs")

def clean_data():
    print("üî• D√©marrage du nettoyage (Mode : Business Data Only)...")
    print("   (Les Users et Prompts seront conserv√©s)")

    # --- 1. NETTOYAGE LOGS ---
    if LOG_DIR.exists():
        print("üìù Nettoyage des logs fichiers...")
        for log_file in LOG_DIR.glob("*.log"):
            with open(log_file, "w") as f:
                f.write("")
            print(f"   - Vid√© : {log_file.name}")

    # --- 2. NETTOYAGE SQLITE (CIBL√â) ---
    print("üóÑÔ∏è  Nettoyage SQLite (Docs & Logs)...")
    with Session(engine) as session:
        # On supprime uniquement les donn√©es m√©tier
        deleted_docs = session.query(DocModel).delete()
        deleted_logs = session.query(ApiLogModel).delete()
        session.commit()
        print(f"‚úÖ Supprim√© : {deleted_docs} documents")
        print(f"‚úÖ Supprim√© : {deleted_logs} logs d'API")
        print("‚ÑπÔ∏è  Users et Prompts conserv√©s.")

    # --- 3. NETTOYAGE CHROMA ---
    print("üé® Reset ChromaDB (Vecteurs)...")
    try:
        client = get_chroma_client()
        try:
            client.delete_collection("rag_chunks")
            print("‚úÖ Collection 'rag_chunks' supprim√©e.")
        except ValueError:
            pass # N'existait pas
        
        # On recr√©e vide
        client.get_or_create_collection("rag_chunks")
        print("‚úÖ Collection 'rag_chunks' recr√©√©e vide.")
        
    except Exception as e:
        print(f"‚ùå Erreur Chroma : {e}")

    print("\nüßπ Nettoyage termin√© ! Pr√™t pour une nouvelle ingestion.")

if __name__ == "__main__":
    clean_data()

# ------------------------------------------
# FICHIER : scripts\concat_code.py
# ------------------------------------------
import os
from pathlib import Path
from datetime import datetime

# --- CONFIGURATION ---
BASE_DIR = Path(__file__).parent.parent # Racine du projet (rag_local)
OUTPUT_DIR = Path("docs/concat")

# Liste exhaustive des dossiers √† scanner
TARGETS = [
    {
        "name": "database", 
        "paths": ["database", "database/models"] # <-- Ajout de models
    },
    {
        "name": "schemas", 
        "paths": ["schemas"]
    },
    {
        "name": "repositories", 
        "paths": ["repositories"]
    },
    {
        "name": "utils", 
        "paths": ["utils"]
    },
    {
        "name": "services_core", 
        "paths": ["services"] # llm.py, ingestion.py...
    },
    {
        "name": "services_chunking", 
        "paths": ["services/chunking", "services/chunking/strategies"] # <-- Ajout des strat√©gies
    },
    {
        "name": "api", 
        "paths": ["api", "api/routes"] # <-- Ajout des routes
    },
    {
        "name": "scripts", 
        "paths": ["scripts"]
    },
]

IGNORE_DIRS = ["__pycache__", ".venv", "venv", ".git", ".vscode", "logs", "docs", "data"]
IGNORE_FILES = [".DS_Store", "local_database.db", ".env"]

def generate_tree(dir_path: Path, prefix: str = ""):
    output = ""
    try:
        # Trier : Dossiers d'abord, puis fichiers
        items = sorted(list(dir_path.iterdir()), key=lambda x: (not x.is_dir(), x.name.lower()))
        
        filtered_items = [
            i for i in items 
            if i.name not in IGNORE_DIRS 
            and i.name not in IGNORE_FILES
            and not i.name.endswith(".pyc")
        ]

        for index, item in enumerate(filtered_items):
            is_last = (index == len(filtered_items) - 1)
            connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
            output += f"{prefix}{connector}{item.name}\n"

            if item.is_dir():
                extension = "    " if is_last else "‚îÇ   "
                output += generate_tree(item, prefix + extension)
    except PermissionError:
        pass
    return output

def concat_files():
    if not OUTPUT_DIR.exists():
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    print("üöÄ D√©but de la concat√©nation Python (Compl√®te)...\n")

    # 1. G√©n√©ration Arborescence
    print("--- G√©n√©ration Arborescence ---")
    tree_str = f"PROJET PYTHON LOCAL\nG√©n√©r√© le : {datetime.now()}\n\nracine\n"
    tree_str += generate_tree(BASE_DIR)
    
    with open(OUTPUT_DIR / "arborescence.txt", "w", encoding="utf-8") as f:
        f.write(tree_str)
    print("‚úÖ arborescence.txt g√©n√©r√©.")

    # 2. Concat√©nation par groupe
    print("\n--- Concat√©nation des sources ---")
    
    for target in TARGETS:
        out_file = OUTPUT_DIR / f"{target['name']}.py.txt"
        
        file_count = 0
        content_buffer = f"# ==========================================\n# GROUPE : {target['name']}\n# ==========================================\n\n"

        for rel_path in target['paths']:
            abs_path = BASE_DIR / rel_path
            
            if not abs_path.exists():
                # print(f"‚ö†Ô∏è Chemin introuvable (ignor√©) : {rel_path}")
                continue

            # On scanne les fichiers .py dans ce dossier pr√©cis
            files = list(abs_path.glob("*.py"))
            files.sort()

            for file_path in files:
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        code = f.read()
                    
                    relative_name = file_path.relative_to(BASE_DIR)
                    content_buffer += f"\n# ------------------------------------------\n"
                    content_buffer += f"# FICHIER : {relative_name}\n"
                    content_buffer += f"# ------------------------------------------\n"
                    content_buffer += code + "\n"
                    file_count += 1
                except Exception as e:
                    print(f"Erreur lecture {file_path}: {e}")

        if file_count > 0:
            with open(out_file, "w", encoding="utf-8") as f:
                f.write(content_buffer)
            print(f"‚úÖ {target['name']} ({file_count} fichiers)")
        else:
            print(f"‚ÑπÔ∏è  {target['name']} (Vide)")

    print("\nüèÅ Termin√© ! V√©rifiez le dossier docs/concat/")

if __name__ == "__main__":
    concat_files()

# ------------------------------------------
# FICHIER : scripts\debug_search.py
# ------------------------------------------
import requests
import json

def debug_search():
    url = "http://127.0.0.1:8000/search/"
    payload = {
        "query": "Goldorak",
        "employee": "admin_ui",
        "limit": 3
    }

    print(f"üîç Recherche brute sur : {url}")
    try:
        r = requests.post(url, json=payload)
        if r.status_code == 200:
            data = r.json()
            print(f"‚úÖ {data['count']} r√©sultats trouv√©s.\n")
            for res in data['results']:
                print(f"--- Score: {res['score_distance']} ---")
                print(f"CONTENU : {res['content']}")
                print(f"METADATA : {res['metadata']}\n")
        else:
            print(f"‚ùå Erreur {r.status_code} : {r.text}")
    except Exception as e:
        print(f"‚ùå Erreur connexion : {e}")

if __name__ == "__main__":
    debug_search()

# ------------------------------------------
# FICHIER : scripts\export_data.py
# ------------------------------------------
import sys
import os
import json
import datetime
from sqlalchemy.orm import Session
from pathlib import Path

# Path hack
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine, get_chroma_client
from database.models import LoginModel, DocModel, ApiLogModel, PromptModel, Base

# Configuration
EXPORT_PATH = Path("docs/export_python")

# Helper pour g√©rer les dates dans le JSON
def json_serial(obj):
    if isinstance(obj, (datetime.datetime, datetime.date)):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def sqlalchemy_to_dict(obj):
    """Convertit un objet SQLAlchemy en dictionnaire propre"""
    return {c.name: getattr(obj, c.name) for c in obj.__table__.columns}

def export_data():
    print("üì¶ D√©marrage de l'export Data (SQLite + Chroma)...")
    
    # Cr√©ation dossier
    if not EXPORT_PATH.exists():
        EXPORT_PATH.mkdir(parents=True, exist_ok=True)
        print(f"üìÇ Dossier cr√©√© : {EXPORT_PATH}")

    # 1. EXPORT SQLITE
    with Session(engine) as session:
        # Docs
        docs = session.query(DocModel).all()
        docs_data = [sqlalchemy_to_dict(d) for d in docs]
        with open(EXPORT_PATH / "Docs.json", "w", encoding="utf-8") as f:
            json.dump(docs_data, f, default=json_serial, indent=2, ensure_ascii=False)
        print(f"‚úÖ [SQLite] Docs export√©s : {len(docs_data)}")

        # Logins
        users = session.query(LoginModel).all()
        users_data = [sqlalchemy_to_dict(u) for u in users]
        with open(EXPORT_PATH / "Logins.json", "w", encoding="utf-8") as f:
            json.dump(users_data, f, default=json_serial, indent=2, ensure_ascii=False)
        print(f"‚úÖ [SQLite] Logins export√©s : {len(users_data)}")

        # Prompts
        prompts = session.query(PromptModel).all()
        prompts_data = [sqlalchemy_to_dict(p) for p in prompts]
        with open(EXPORT_PATH / "Prompts.json", "w", encoding="utf-8") as f:
            json.dump(prompts_data, f, default=json_serial, indent=2, ensure_ascii=False)
        print(f"‚úÖ [SQLite] Prompts export√©s : {len(prompts_data)}")

    # 2. EXPORT CHROMA (Chunks)
    try:
        chroma_client = get_chroma_client()
        collection = chroma_client.get_collection("rag_chunks")
        # On r√©cup√®re tout
        results = collection.get()
        
        # Reformatage pour lisibilit√©
        chunks_data = []
        if results["ids"]:
            for i, id_val in enumerate(results["ids"]):
                chunks_data.append({
                    "id": id_val,
                    "content": results["documents"][i],
                    "metadata": results["metadatas"][i]
                })

        with open(EXPORT_PATH / "Chunks_Chroma.json", "w", encoding="utf-8") as f:
            json.dump(chunks_data, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ [Chroma] Chunks export√©s : {len(chunks_data)}")

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur export Chroma (collection vide ?) : {e}")

    print(f"\nüéâ Export termin√© dans : {EXPORT_PATH.absolute()}")

if __name__ == "__main__":
    export_data()

# ------------------------------------------
# FICHIER : scripts\init_login.py
# ------------------------------------------
import sys
import os

# Ajout du dossier parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import get_db
from database.models import LoginModel

def seed_user():
    print("üöÄ D√©marrage de l'initialisation utilisateur...")

    db = next(get_db())
    target_email = "cv@duhamel.xyz"
    
    try:
        # --- [1] Nettoyage pr√©ventif ---
        # On supprime l'utilisateur s'il existe d√©j√† pour √©viter les doublons/erreurs
        deleted_count = db.query(LoginModel).filter(LoginModel.employee == target_email).delete()
        db.commit()
        if deleted_count > 0:
            print(f"üßπ Utilisateur existant '{target_email}' supprim√©.")

        # --- [2] Insertion de donn√©es ---
        new_user = LoginModel(
            employee=target_email,
            company="Local Corp",
            lastname="Dev",
            firstname="Junior",
            password="admin1",
            credit=1000,
            services={"graph_rag": True}
        )
        
        db.add(new_user)
        db.commit()
        db.refresh(new_user)
        
        print(f"‚úÖ User ins√©r√© en DB avec succ√®s : ID={new_user.id}, Services={new_user.services}")

    except Exception as e:
        db.rollback() # Important en cas d'erreur
        print(f"‚ùå Erreur lors de l'op√©ration SQL : {e}")
    finally:
        db.close() # Bonne pratique pour lib√©rer la connexion

if __name__ == "__main__":
    seed_user()

# ------------------------------------------
# FICHIER : scripts\seed_prompts.py
# ------------------------------------------
import sys
import os
import json

# Path hack
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy import text
from database.connection import engine
from database.models import Base
from repositories.prompt import prompt_repository
from schemas.prompt import PromptCreate

# VOS DONN√âES JSON (Copi√©es depuis votre fichier)
RAW_PROMPTS_DATA = [
{
    "name": "synthesis_tags",
    "user": "system",
    "prompt": "# R√îLE\nTu es un expert en analyse documentaire.\n\n# MISSION\nAnalyse le document JSON/Texte et g√©n√®re une synth√®se structur√©e.\n\n# SORTIE (JSON UNIQUEMENT)\n{\n  \"synthesis\": \"R√©sum√© en ANGLAIS de 15 lignes max. Identifie le type (Profil, Entreprise, Article).\",\n  \"suggested_tags\": [\"tag1\", \"tag2\"] (Max 5, anglais, minuscule, sans espace)\n}"
  },
  {
    "name": "agent_rerank",
    "user": "system",
    "prompt": "Tu es un juge de pertinence strict.\n√âvalue les chunks par rapport √† la QUESTION : \"{question}\"\n\nCONTEXTE :\n{context}\n\n# R√àGLES DE SCORING (0.0 √† 1.0)\n\n1. **BRUIT = 0.0** : Si le chunk est une liste de liens, 'Related Articles', 'See Also', un pied de page, des mentions l√©gales ou une pub.\n2. **CORRESPONDANCE EXACTE = 1.0** : Si la r√©ponse ou les m√©tadonn√©es (dates, noms) correspondent parfaitement.\n3. **PERTINENT = 0.6 - 0.9** : Le contexte est bon.\n4. **HORS SUJET = 0.1** : Aucun rapport.\n\n# SORTIE (JSON UNIQUEMENT)\n[\n  { \"chunk_index\": 0, \"score\": 0.0 },\n  { \"chunk_index\": 1, \"score\": 0.9 }\n]"
  },
  {
    "name": "chunk_post",
    "user": "system",
    "prompt": "# MISSION\nExtrais les m√©tadonn√©es de ce post LinkedIn.\n\n# TEXTE\n{text}\n\n# SORTIE (JSON UNIQUEMENT)\n{\n  \"metadata\": {\n    \"name\": \"...\",\n    \"company\": \"...\",\n    \"industry\": \"...\",\n    \"location\": \"...\",\n    \"type\": \"general | experience | education | post\"\n  }\n}"
  },
  {
    "name": "chunk_about",
    "user": "system",
    "prompt": "# MISSION\nExtrais les comp√©tences et infos cl√©s de ce texte 'About'.\n\n# SORTIE (JSON UNIQUEMENT)\n{\n  \"metadata\": {\n    \"hard_skills\": [\"...\"],\n    \"soft_skills\": [\"...\"],\n    \"technologies\": [\"...\"],\n    \"projects\": [\"...\"]\n  }\n}"
  },
  {
    "name": "chunk_hypothetical_questions",
    "user": "system",
    "prompt": "G√©n√®re 3 questions courtes auxquelles ce texte r√©pond :\n\n\"\"\"\n${text}\n\"\"\"\n\nFormat : Une question par ligne, sans puce ni num√©ro."
  },
  {
    "name": "chunk_entities",
    "user": "system",
    "prompt": "# MISSION\nExtrais les entit√©s nomm√©es.\n\nTEXTE:\n{text}\n\n# SORTIE (JSON)\n{\n  \"people\": [{ \"name\": \"...\" }],\n  \"companies\": [{ \"name\": \"...\" }],\n  \"locations\": [{ \"name\": \"...\" }],\n  \"tools\": [{ \"name\": \"...\" }]\n}"
  },
  {
    "name": "graph_extraction",
    "user": "system",
    "prompt": "Extrais les relations (triplets) du texte.\n\n# SORTIE (JSON)\n{\n  \"triplets\": [\n    {\n      \"source\": \"Entit√©1\",\n      \"source_type\": \"PERSON|COMPANY\",\n      \"relation\": \"VERBE_MAJUSCULE\",\n      \"target\": \"Entit√©2\",\n      \"target_type\": \"LOCATION|TOOL\"\n    }\n  ]\n}"
  },
    {
    "name": "three-pass",
    "user": "cv@duhamel.xyz",
    "prompt": "Pass 1: a quick skim of what the paper is about.\nPass 2: the main ideas and why they matter.\nPass 3: the deeper details I should pay attention to.\nJE VEUX TES REPONSES EN FRAN√áAIS"
  }
]

def seed_prompts():
    print("üöÄ Initialisation des Prompts (Seed)...")
    
    # S'assurer que la table existe
    Base.metadata.create_all(bind=engine)
    
    print("üßπ Nettoyage de la table 'prompts' existante...")
    try:
        with engine.connect() as connection:
            connection.execute(text("DELETE FROM prompts"))
            connection.commit()
            print("üóëÔ∏è Table vid√©e avec succ√®s.")
    except Exception as e:
        print(f"‚ö†Ô∏è Attention : Impossible de vider la table (elle est peut-√™tre d√©j√† vide ou le nom diff√®re) : {e}")

    count = 0
    for p_data in RAW_PROMPTS_DATA:
        try:
            # On ignore l'ID Mongo (_id) s'il est pr√©sent, on laisse SQLite g√©rer
            prompt_in = PromptCreate(
                name=p_data["name"],
                prompt=p_data["prompt"],
                user=p_data["user"],
                description=p_data.get("description")
            )
            prompt_repository.upsert_prompt(prompt_in)
            count += 1
            print(f"‚úÖ Prompt trait√© : {p_data['name']}")
        except Exception as e:
            print(f"‚ùå Erreur sur {p_data.get('name')}: {e}")

    print(f"\nüéâ Termin√© ! {count} prompts sont maintenant en base SQLite.")

if __name__ == "__main__":
    seed_prompts()

# ------------------------------------------
# FICHIER : scripts\test_01_check_state.py
# ------------------------------------------
import sys
import os
import requests
from sqlalchemy.orm import Session

# Ajout du dossier parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine, get_chroma_client
from database.models import PromptModel, LoginModel, DocModel

# Liste des prompts critiques pour que le RAG fonctionne
REQUIRED_PROMPTS = ["synthesis_tags", "chunk_hypothetical_questions", "chunk_entities", "graph_extraction"]

def check_system_state():
    print("üîç [DIAGNOSTIC] V√©rification de l'√©tat du syst√®me...\n")
    
    with Session(engine) as db:
        # 1. V√©rification des Prompts (CRITIQUE)
        print("1Ô∏è‚É£  V√©rification des Prompts...")
        existing_prompts = [p.name for p in db.query(PromptModel).all()]
        missing = [p for p in REQUIRED_PROMPTS if p not in existing_prompts]
        
        if missing:
            print(f"   ‚ùå ALERTE : Il manque ces prompts critiques : {missing}")
            print("      -> Lancez 'python scripts/seed_prompts.py' si vous les avez perdus.")
        else:
            print(f"   ‚úÖ {len(existing_prompts)} prompts trouv√©s (dont les {len(REQUIRED_PROMPTS)} critiques).")

        # 2. V√©rification des Docs (Doit √™tre vide ou presque)
        count_docs = db.query(DocModel).count()
        print(f"\n2Ô∏è‚É£  V√©rification des Documents : {count_docs} documents en base SQLite.")

    # 3. V√©rification ChromaDB
    print("\n3Ô∏è‚É£  V√©rification ChromaDB (Vecteurs)...")
    try:
        client = get_chroma_client()
        coll = client.get_or_create_collection("rag_chunks")
        count_vectors = coll.count()
        print(f"   ‚úÖ Connexion Chroma OK. Contient {count_vectors} chunks.")
    except Exception as e:
        print(f"   ‚ùå ERREUR CHROMA : {e}")

    print("\nüèÅ Diagnostic termin√©.")

if __name__ == "__main__":
    check_system_state()

# ------------------------------------------
# FICHIER : scripts\test_02_full_flow.py
# ------------------------------------------
import requests
import time
import sys
import json

# Configuration
API_URL = "http://127.0.0.1:8000"
TEST_USER = "admin_test"
TEST_DOC_TEXT = "Le projet secret s'appelle 'Projet Omega'. Il vise √† cr√©er une IA quantique d'ici 2030."

def run_test():
    print(f"üöÄ D√©marrage du Test Fonctionnel sur {API_URL}...\n")

    # ---------------------------------------------------------
    # 1. INGESTION (On pousse la donn√©e)
    # ---------------------------------------------------------
    print("üîπ [ETAPE 1] Ingestion du document...")
    try:
        res = requests.post(f"{API_URL}/ingest/text", json={
            "text": TEST_DOC_TEXT,
            "tags": ["test", "secret"],
            "employee": TEST_USER
        })
        if res.status_code != 200:
            print(f"   ‚ùå Erreur API Ingest ({res.status_code}) : {res.text}")
            return
        
        data = res.json()
        print(f"   ‚úÖ Ingestion OK. ID: {data.get('doc_id')} | Chunks cr√©√©s: {data.get('chunks_count')}")
        
        if data.get('chunks_count') == 0:
            print("   ‚ö†Ô∏è  ATTENTION : 0 chunks cr√©√©s ! V√©rifiez 'services/chunking/manager.py'.")
            return

    except Exception as e:
        print(f"   ‚ùå Exception critique : {e}")
        return

    print("   ‚è≥ Attente de 2 secondes pour l'indexation...")
    time.sleep(2)

    # ---------------------------------------------------------
    # 2. VERIFICATION SEARCH (On v√©rifie la pr√©sence physique)
    # ---------------------------------------------------------
    print("\nüîπ [ETAPE 2] V√©rification Vectorielle (Search)...")
    try:
        # On utilise la route /search que vous avez ajout√©e dans api.py
        res = requests.post(f"{API_URL}/search/", json={
            "query": "Omega",
            "limit": 3,
            "employee": TEST_USER
        })
        
        if res.status_code == 200:
            results = res.json().get("results", [])
            found = False
            for r in results:
                if "Omega" in r.get("content", ""):
                    print(f"   ‚úÖ Contenu retrouv√© dans Chroma ! (Score: {r.get('score_distance')})")
                    print(f"      Extrait : {r.get('content')[:50]}...")
                    found = True
                    break
            
            if not found:
                print("   ‚ùå ECHEC : Le document n'est pas remont√© par la recherche vectorielle.")
                print(f"      R√©sultats bruts : {json.dumps(results, indent=2)}")
                return
        else:
            print(f"   ‚ùå Erreur API Search ({res.status_code}) : {res.text}")
            return

    except Exception as e:
        print(f"   ‚ùå Erreur lors du test Search : {e}")
        return

    # ---------------------------------------------------------
    # 3. TEST CHAT (On teste l'intelligence)
    # ---------------------------------------------------------
    print("\nüîπ [ETAPE 3] Test du Chat (RAG)...")
    question = "Quel est le nom du projet secret et son but ?"
    try:
        res = requests.post(f"{API_URL}/chat/", json={
            "question": question,
            "employee": TEST_USER
        })
        
        if res.status_code == 200:
            answer = res.json().get("answer", "")
            sources = res.json().get("sources", [])
            
            print(f"   ‚ùì Question : {question}")
            print(f"   ü§ñ R√©ponse  : {answer}\n")
            
            if "Omega" in answer:
                print("   üéâ SUCC√àS TOTAL : Le syst√®me fonctionne de bout en bout !")
            else:
                print("   üî∏ RESULTAT MITIG√â : Le document est trouv√© mais le LLM ne l'a pas utilis√©.")
                print("      -> V√©rifiez le Prompt Syst√®me dans 'services/chat.py'.")
        else:
            print(f"   ‚ùå Erreur API Chat ({res.status_code}) : {res.text}")

    except Exception as e:
        print(f"   ‚ùå Erreur lors du test Chat : {e}")

if __name__ == "__main__":
    run_test()

# ------------------------------------------
# FICHIER : scripts\test_master.py
# ------------------------------------------
import requests
import time
import os
import sys

# Configuration
API_URL = "http://127.0.0.1:8000"
EMPLOYEE_ID = "admin_test_master"

# Codes couleurs pour la lisibilit√©
GREEN = "\033[92m"
RED = "\033[91m"
RESET = "\033[0m"

def log(step, success, message):
    icon = "‚úÖ" if success else "‚ùå"
    color = GREEN if success else RED
    print(f"{color}{icon} [{step}] {message}{RESET}")
    if not success:
        print(f"{RED}   -> ARR√äT CRITIQUE : Corrigez cette √©tape avant de continuer.{RESET}")
        sys.exit(1)

def create_dummy_pdf():
    filename = "test_doc.pdf"
    with open(filename, "wb") as f:
        f.write(b"%PDF-1.4\n1 0 obj\n<< /Type /Catalog /Pages 2 0 R >>\nendobj\n2 0 obj\n<< /Type /Pages /Kids [3 0 R] /Count 1 >>\nendobj\n3 0 obj\n<< /Type /Page /Parent 2 0 R /Resources << >> /MediaBox [0 0 612 792] /Contents 4 0 R >>\nendobj\n4 0 obj\n<< /Length 50 >>\nstream\nBT /F1 12 Tf 72 720 Td (Ceci est un test PDF pour le RAG.) Tj ET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000010 00000 n \n0000000060 00000 n \n0000000157 00000 n \n0000000296 00000 n \ntrailer\n<< /Size 5 /Root 1 0 R >>\nstartxref\n400\n%%EOF")
    return filename

def run_tests():
    print(f"\nüöÄ D√âMARRAGE BATTERIE DE TESTS SUR {API_URL}\n")

    # ---------------------------------------------------------
    # 1. TEST CONNEXION (Healthcheck)
    # ---------------------------------------------------------
    try:
        r = requests.get(f"{API_URL}/")
        log("API", r.status_code == 200, f"API en ligne (Ping: {r.json().get('status')})")
    except Exception as e:
        log("API", False, f"Impossible de joindre l'API. Lancez 'python -m api.main'. Erreur: {e}")

    # ---------------------------------------------------------
    # 2. TEST INGESTION TEXTE (Route: POST /ingest/text)
    # ---------------------------------------------------------
    print("\nüîπ TEST 2 : Ingestion Texte")
    payload_text = {
        "text": "Le projet Apollo 11 a permis √† l'homme de marcher sur la Lune en 1969.",
        "tags": ["espace", "histoire"],
        "employee": EMPLOYEE_ID
    }
    r = requests.post(f"{API_URL}/ingest/text", json=payload_text)
    
    if r.status_code == 200:
        data = r.json()
        log("INGEST_TEXT", True, f"Doc ID: {data.get('doc_id')} | Chunks: {data.get('chunks_count')}")
    else:
        log("INGEST_TEXT", False, f"Erreur {r.status_code}: {r.text}")

    # ---------------------------------------------------------
    # 3. TEST INGESTION URL (Route: POST /ingest/url)
    # ---------------------------------------------------------
    print("\nüîπ TEST 3 : Ingestion URL")
    # URL Wikip√©dia simple pour √©viter les blocages
    payload_url = {
        "url": "https://fr.wikipedia.org/wiki/Lune",
        "tags": ["wiki", "espace"],
        "employee": EMPLOYEE_ID
    }
    r = requests.post(f"{API_URL}/ingest/url", json=payload_url)

    if r.status_code == 200:
        data = r.json()
        log("INGEST_URL", True, f"Doc ID: {data.get('doc_id')} | Chunks: {data.get('chunks_count')}")
    else:
        # On tol√®re l'√©chec si Tavily n'est pas configur√©, mais on le signale
        if "TAVILY_API_KEY" not in os.environ and "400" in str(r.status_code):
             print(f"{RED}‚ö†Ô∏è  Echec URL : V√©rifiez votre TAVILY_API_KEY dans .env{RESET}")
        else:
            log("INGEST_URL", False, f"Erreur {r.status_code}: {r.text}")

    # ---------------------------------------------------------
    # 4. TEST INGESTION FICHIER (Route: POST /ingest/file)
    # ---------------------------------------------------------
    print("\nüîπ TEST 4 : Ingestion Fichier (Upload)")
    pdf_name = create_dummy_pdf()
    try:
        with open(pdf_name, "rb") as f:
            files = {"file": (pdf_name, f, "application/pdf")}
            data = {"tags": "pdf,test", "employee": EMPLOYEE_ID}
            r = requests.post(f"{API_URL}/ingest/file", files=files, data=data)
        
        if r.status_code == 200:
            res_json = r.json()
            log("INGEST_FILE", True, f"Doc ID: {res_json.get('doc_id')}")
        else:
            log("INGEST_FILE", False, f"Erreur {r.status_code}: {r.text}")
    finally:
        if os.path.exists(pdf_name):
            os.remove(pdf_name)

    print("‚è≥ Pause 2s pour indexation ChromaDB...")
    time.sleep(2)

    # ---------------------------------------------------------
    # 5. TEST RECHERCHE (Route: POST /search/)
    # ---------------------------------------------------------
    print("\nüîπ TEST 5 : Recherche Vectorielle (Debug)")
    payload_search = {
        "query": "Apollo 11",
        "limit": 3,
        "employee": EMPLOYEE_ID
    }
    r = requests.post(f"{API_URL}/search/", json=payload_search)
    
    if r.status_code == 200:
        results = r.json().get("results", [])
        if results and len(results) > 0:
            log("SEARCH", True, f"Trouv√© : {results[0]['content'][:50]}... (Score: {results[0]['score_distance']})")
        else:
            log("SEARCH", False, "Aucun r√©sultat trouv√© (Indexation √©chou√©e ?)")
    else:
        log("SEARCH", False, f"Erreur {r.status_code}: {r.text}")

    # ---------------------------------------------------------
    # 6. TEST CHAT (Route: POST /chat/)
    # ---------------------------------------------------------
    print("\nüîπ TEST 6 : Chat RAG")
    payload_chat = {
        "question": "Quand l'homme a-t-il march√© sur la Lune ?",
        "employee": EMPLOYEE_ID
    }
    r = requests.post(f"{API_URL}/chat/", json=payload_chat)

    if r.status_code == 200:
        ans = r.json()
        response_text = ans.get("answer", "")
        sources = ans.get("sources", [])
        print(f"   ü§ñ R√©ponse : {response_text}")
        print(f"   üìö Sources : {sources}")
        
        if "1969" in response_text or "Apollo" in response_text:
            log("CHAT", True, "R√©ponse coh√©rente avec le contexte.")
        else:
            print(f"{RED}‚ö†Ô∏è  R√©ponse IA faible (V√©rifiez le prompt ou le mod√®le Ollama).{RESET}")
            # On ne bloque pas ici, car techniquement l'API a r√©pondu
    else:
        log("CHAT", False, f"Erreur {r.status_code}: {r.text}")

    print(f"\n{GREEN}üéâ BATTERIE DE TESTS TERMIN√âE AVEC SUCC√àS !{RESET}")
    print("Votre API Python fonctionne parfaitement.")

if __name__ == "__main__":
    run_tests()

# ------------------------------------------
# FICHIER : scripts\validate_chunking_profile.py
# ------------------------------------------
import sys
import os
import time
import logging

# Config logs
logging.basicConfig(level=logging.INFO)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from services.chat import chat_service

def run_profile_test():
    print("üöÄ D√©marrage Test Chunking (Profil Structur√©)...")
    Base.metadata.create_all(bind=engine)
    
    employee_id = "recruiter_bob"
    
    # 1. Donn√©e structur√©e (Comme si elle venait d'un scraping LinkedIn)
    fake_profile = {
        "name": "Alice Wonderland",
        "about": "Experte en Data Science passionn√©e par les lapins blancs.",
        "experience": [
            {
                "title": "Lead Data Scientist",
                "company": "Wonder Corp",
                "date_range": "2020 - Present",
                "description": "Gestion de l'√©quipe IA, d√©ploiement de mod√®les LLM."
            },
            {
                "title": "Junior Analyst",
                "company": "Rabbit Hole Inc",
                "date_range": "2018 - 2020",
                "description": "Analyse de donn√©es temporelles."
            }
        ],
        "education": [
            {
                "school": "University of Hearts",
                "degree": "Master in Magic"
            }
        ]
    }

    # 2. Ingestion (on passe le dict direct)
    print("\n--- [1] Ingestion du Profil JSON ---")
    result = ingestion_service.process_input(
        input_data=fake_profile,
        employee=employee_id,
        tags=["candidat", "data"],
        origin="api_push"
    )
    
    print(f"‚úÖ Ingestion termin√©e.")
    print(f"   Strat√©gie utilis√©e : {result['strategy']}")
    print(f"   Chunks cr√©√©s : {result['chunks_count']}")
    
    # On attend 3 chunks logiques : 1 About + 2 Experiences + 1 Education = 4 chunks
    # (Ou 3 si about est petit)
    
    time.sleep(1)

    # 3. Chat (V√©rification RAG)
    print("\n--- [2] Question sur le CV ---")
    # On pose une question pr√©cise sur une exp√©rience
    question = "Qu'a fait Alice chez Rabbit Hole Inc ?"
    
    answer = chat_service.chat(question, employee_id)
    
    print(f"ü§ñ R√©ponse IA :\n{answer['response']}")
    print(f"üìö Sources : {answer['sources']}")

if __name__ == "__main__":
    run_profile_test()

# ------------------------------------------
# FICHIER : scripts\validate_enrichment.py
# ------------------------------------------
import sys
import os
import logging
import json

# Config logs
logging.basicConfig(level=logging.INFO)

# Ajout du dossier parent au path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# CORRECTION ICI : On importe engine et Base au lieu de init_db
from database.connection import engine
from database.models import Base
from services.chunking.enrichment import enrichment_service

def run_enrichment_test():
    print("üöÄ D√©marrage Test Enrichissement LLM...")
    
    # CORRECTION ICI : Initialisation explicite via SQLAlchemy
    Base.metadata.create_all(bind=engine)

    # 1. Texte "About" type LinkedIn
    about_text = """
    D√©veloppeur Full Stack avec 5 ans d'exp√©rience. 
    Expert en Python, FastAPI et React. 
    J'ai g√©r√© des projets cloud sur AWS et Azure. 
    Je suis reconnu pour ma capacit√© √† r√©soudre des probl√®mes complexes et mon esprit d'√©quipe.
    Certifi√© AWS Solutions Architect.
    """

    print(f"\n--- Texte √† analyser ---\n{about_text.strip()}\n")

    # 2. Appel du service (utilise le prompt 'chunk_about' en base)
    try:
        print("‚è≥ Appel au LLM (Ollama) via EnrichmentService...")
        metadata = enrichment_service.extract_metadata(about_text, "chunk_about")
        
        print("\n‚úÖ R√©sultat Structur√© (JSON) :")
        print("-" * 30)
        print(json.dumps(metadata, indent=2, ensure_ascii=False))
        print("-" * 30)

        # V√©rifications
        if metadata and ("hard_skills" in metadata or "technologies" in metadata):
            print("üéâ SUCC√àS : Des comp√©tences ont √©t√© extraites !")
        else:
            print("‚ö†Ô∏è AVERTISSEMENT : Le JSON est valide mais vide. V√©rifiez que le prompt 'chunk_about' est bien en base (seed_prompts.py).")

    except Exception as e:
        print(f"‚ùå ERREUR : {e}")

if __name__ == "__main__":
    run_enrichment_test()

# ------------------------------------------
# FICHIER : scripts\validate_ingest_file.py
# ------------------------------------------
import sys
import os
import time
from dotenv import load_dotenv
from fpdf import FPDF # Pour g√©n√©rer un PDF de test rapidement

# pip install fpdf (juste pour ce script de test) ou cr√©ez un fichier manuellement

load_dotenv()
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from services.chat import chat_service

def create_dummy_pdf(filename):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt="RAPPORT CONFIDENTIEL - PROJET ORION", ln=1, align="C")
    pdf.cell(200, 10, txt="1. Le budget allou√© est de 50 millions d'euros.", ln=2)
    pdf.cell(200, 10, txt="2. Le directeur du projet est M. Vector.", ln=3)
    pdf.output(filename)
    print(f"üìÑ PDF de test g√©n√©r√© : {filename}")

def run_file_test():
    print("üöÄ D√©marrage Test Ingestion FICHIER (PDF)...")
    Base.metadata.create_all(bind=engine)
    employee_id = "file_tester"
    
    # 1. G√©n√©ration d'un fichier PDF local
    pdf_path = "test_projet_orion.pdf"
    try:
        create_dummy_pdf(pdf_path)
    except ImportError:
        print("‚ö†Ô∏è 'fpdf' manquant. Installez-le (pip install fpdf) ou cr√©ez 'test_projet_orion.pdf' manuellement.")
        return

    # 2. Ingestion
    print(f"\n--- [1] Ingestion du fichier : {pdf_path} ---")
    try:
        # On passe le CHEMIN du fichier
        result = ingestion_service.process_input(
            input_data=pdf_path,
            employee=employee_id,
            tags=["projet", "finance"],
            origin="filesystem"
        )
        print(f"‚úÖ Ingestion Fichier r√©ussie !")
        print(f"   Source d√©tect√©e : {result['source']}")
        print(f"   Doc ID : {result['doc_id']}")
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        return

    # Pause Indexation
    time.sleep(1)

    # 3. Chat
    print("\n--- [2] V√©rification RAG sur le PDF ---")
    question = "Quel est le budget du projet Orion et qui est le directeur ?"
    
    answer = chat_service.chat(question, employee_id)
    
    print(f"ü§ñ R√©ponse IA :\n{answer['response']}")
    print(f"üìö Sources : {answer['sources']}")

    # Nettoyage
    if os.path.exists(pdf_path):
        os.remove(pdf_path)

if __name__ == "__main__":
    run_file_test()

# ------------------------------------------
# FICHIER : scripts\validate_ingest_url.py
# ------------------------------------------
import sys
import os
import time
import logging # <--- Import n√©cessaire

# Configuration des logs pour voir ce que dit Tavily
logging.basicConfig(level=logging.INFO)

# Ajout du dossier parent au path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from services.chat import chat_service

def run_url_test():
    print("üöÄ D√©marrage Test Ingestion URL (Tavily)...")
    
    # Init DB
    Base.metadata.create_all(bind=engine)

    employee_id = "web_tester"
    
    # URL de test : On utilise Wikip√©dia pour valider la connexion API (plus fiable que TDS)
    target_url = "https://fr.wikipedia.org/wiki/Intelligence_artificielle" 

    print(f"\n--- [1] Tentative d'ingestion de : {target_url} ---")
    try:
        # On appelle la nouvelle m√©thode unifi√©e 'process_input'
        result = ingestion_service.process_input(
            input_data=target_url,
            employee=employee_id,
            tags=["ia", "wiki", "web"],
            origin="test_script"
        )
        print(f"‚úÖ Ingestion Web r√©ussie !")
        print(f"   Source : {result['source']}")
        print(f"   Chunks : {result['chunks_count']}")
        
    except Exception as e:
        print(f"‚ùå Erreur critique : {e}")
        # On ne quitte pas, on veut voir les logs au dessus
        return

    # Pause pour l'indexation Chroma
    time.sleep(2)

    print("\n--- [2] V√©rification RAG sur le contenu Web ---")
    question = "C'est quoi l'intelligence artificielle ?"
    
    answer = chat_service.chat(question, employee_id)
    
    print(f"ü§ñ R√©ponse IA :\n{answer['response']}")
    print(f"üìö Sources : {answer['sources']}")

if __name__ == "__main__":
    run_url_test()

# ------------------------------------------
# FICHIER : scripts\validate_step1.py
# ------------------------------------------
import sys
import os

# Ajout du dossier parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine, get_db
from database.models import Base, LoginModel, DocModel
from schemas.user import LoginCreate, LoginResponse
from schemas.doc import DocCreate

def run_validation():
    print("üöÄ D√©marrage de la validation √âtape 1 (Database & Schemas)...")

    # 1. Cr√©ation des Tables (Test des Models)
    print("\n--- [1] Initialisation SQLite ---")
    try:
        Base.metadata.create_all(bind=engine)
        print("‚úÖ Tables cr√©√©es avec succ√®s (logins, docs, api_logs).")
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation tables: {e}")
        return

    # 2. Insertion de donn√©es (Test Session + Models)
    print("\n--- [2] Test Insertion SQL ---")
    db = next(get_db())
    
    # Nettoyage pr√©ventif
    db.query(LoginModel).filter(LoginModel.employee == "test_dev").delete()
    db.commit()

    try:
        new_user = LoginModel(
            employee="cv@duhamel.xyz",
            company="Local Corp",
            lastname="Dev",
            firstname="Junior",
            password="admin1",
            services={"graph_rag": True}
        )
        db.add(new_user)
        db.commit()
        db.refresh(new_user)
        print(f"‚úÖ User ins√©r√© en DB: ID={new_user.id}, Services={new_user.services}")
    except Exception as e:
        print(f"‚ùå Erreur insertion SQL: {e}")
        return

    # 3. Validation Pydantic (Test Schemas -> Model -> Schema)
    print("\n--- [3] Test Validation Pydantic ---")
    try:
        # Conversion Model SQL -> Schema Pydantic
        user_schema = LoginResponse.model_validate(new_user)
        print(f"‚úÖ Conversion SQL->Pydantic r√©ussie: {user_schema.employee} (Pass masqu√©)")
        
        # Test Doc Schema
        doc_data = DocCreate(
            doc="https://linkedin.com/in/test",
            category="profile",
            source="linkedin",
            origin="extension",
            tags=["test"],
            employee="test_dev",
            job_id="job_123",
            page_content={"name": "Test Profile", "skills": ["Python"]}
        )
        print(f"‚úÖ Schema Doc valide: {doc_data.doc}")
        
    except Exception as e:
        print(f"‚ùå Erreur validation Pydantic: {e}")
        return

    print("\nüéâ √âTAPE 1 TERMIN√âE AVEC SUCC√àS : Structure propre et modulaire valid√©e.")

if __name__ == "__main__":
    run_validation()

# ------------------------------------------
# FICHIER : scripts\validate_step2.py
# ------------------------------------------
import sys
import os

# Path hack pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from schemas.user import LoginCreate
from schemas.doc import DocCreate, Chunk as ChunkSchema
from repositories.user import user_repository
from repositories.doc import doc_repository
from repositories.chunk import chunk_repository

def run_validation():
    print("üöÄ D√©marrage Validation √âtape 2 (Repositories)...")

    # 1. Reset DB
    Base.metadata.drop_all(bind=engine)
    Base.metadata.create_all(bind=engine)
    print("‚úÖ DB Reset OK.")

    # 2. Cr√©ation User
    print("\n--- [User Repo] ---")
    user = user_repository.create_user(LoginCreate(
        employee="jean.dupont@local.fr",
        company="MyLocalCorp",
        lastname="Dupont",
        firstname="Jean",
        password="secret_pass",
        credit=100
    ))
    print(f"‚úÖ User cr√©√©: {user.firstname} {user.lastname}")

    # 3. Cr√©ation Doc (M√©tadonn√©es)
    print("\n--- [Doc Repo] ---")
    doc_data = DocCreate(
        doc="https://fr.wikipedia.org/wiki/Python_(langage)",
        category="wiki",
        source="web",
        origin="import",
        tags=["tech", "python"],
        employee="jean.dupont@local.fr",
        job_id="job_1",
        page_content={"title": "Python Langage", "summary": "Langage de programmation interpr√©t√©."}
    )
    doc = doc_repository.upsert_doc(doc_data)
    print(f"‚úÖ Doc ins√©r√© (SQLite): {doc.doc} (Status: {doc.status})")

    # 4. Ajout Chunks (Vecteurs)
    print("\n--- [Chunk Repo] ---")
    chunks = [
        ChunkSchema(
            content="Python est un langage de programmation interpr√©t√©, multi-paradigme et multiplateformes.",
            metadata={"type": "intro", "page": 1}
        ),
        ChunkSchema(
            content="Il favorise la programmation imp√©rative structur√©e, fonctionnelle et orient√©e objet.",
            metadata={"type": "details", "page": 1}
        )
    ]
    chunk_repository.add_chunks(doc.doc, doc.employee, chunks)
    
    # 5. Test Recherche
    print("\n--- [Search Test] ---")
    results = chunk_repository.search("programmation objet", "jean.dupont@local.fr", limit=1)
    
    if results['documents'] and len(results['documents'][0]) > 0:
        found_text = results['documents'][0][0]
        print(f"‚úÖ Recherche S√©mantique OK.\n   Question: 'programmation objet'\n   Trouv√©: '{found_text}'")
    else:
        print("‚ùå Echec de la recherche s√©mantique.")

if __name__ == "__main__":
    run_validation()

# ------------------------------------------
# FICHIER : scripts\validate_step3.py
# ------------------------------------------
import sys
import os
import time

# Path hack
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from services.chat import chat_service
from repositories.doc import doc_repository

def run_validation():
    print("üöÄ D√©marrage Validation √âtape 3 (Services)...")
    
    # 1. Initialisation de la DB (Correction ici : on utilise engine direct)
    Base.metadata.create_all(bind=engine)
    print("‚úÖ DB Initialis√©e.")

    employee_id = "tester@local.com"
    doc_name = "procedure_securite.txt"
    
    # 2. Texte de test (Contenu m√©tier)
    content = """
    PROC√âDURE DE S√âCURIT√â - CODE ROUGE
    1. En cas d'incendie, ne prenez pas l'ascenseur.
    2. Le point de rassemblement est le Parking Sud, Zone B.
    3. Le code du coffre-fort des serveurs est 'K9-Alpha-77'.
    4. Le responsable de la s√©curit√© est Mme Martin (poste 404).
    """

    # 3. Test Ingestion
    print(f"\n--- [1] Ingestion du document : {doc_name} ---")
    try:
        result = ingestion_service.process_text_document(
            doc_id=doc_name,
            text_content=content,
            employee=employee_id,
            tags=["s√©curit√©", "interne"],
            origin="upload"
        )
        print(f"‚úÖ Ingestion termin√©e. Chunks cr√©√©s : {result['chunks_count']}")
    except Exception as e:
        print(f"‚ùå Erreur Ingestion: {e}")
        return
    
    # V√©rification du statut en base
    doc_in_db = doc_repository.get_doc(doc_name, employee_id)
    if doc_in_db:
        print(f"   Statut en DB : {doc_in_db.status}")
    else:
        print("‚ùå Erreur: Document non trouv√© en base SQLite apr√®s ingestion.")

    # Petite pause pour laisser Chroma indexer
    time.sleep(1)

    # 4. Test Chat (RAG)
    print("\n--- [2] Test du Chat RAG ---")
    question = "Quel est le code du coffre et o√π est le point de rassemblement ?"
    print(f"‚ùì Question : {question}")
    
    try:
        chat_result = chat_service.chat(question, employee_id)
        
        print("\nü§ñ R√©ponse de l'IA :")
        print("-" * 40)
        print(chat_result['response'])
        print("-" * 40)
        print(f"üìö Sources utilis√©es : {chat_result['sources']}")

        # Validation simple du contenu
        if "K9-Alpha-77" in chat_result['response'] or "Parking Sud" in chat_result['response']:
            print("\nüéâ √âTAPE 3 VALID√âE : Le RAG complet fonctionne !")
        else:
            print("\n‚ö†Ô∏è ATTENTION : L'IA n'a pas donn√© la r√©ponse attendue. V√©rifiez que Ollama tourne bien.")
            
    except Exception as e:
        print(f"‚ùå Erreur Chat: {e}")

if __name__ == "__main__":
    run_validation()

# ------------------------------------------
# FICHIER : scripts\verify_chunking_parity.py
# ------------------------------------------
import sys
import os
import time
import logging

logging.basicConfig(level=logging.INFO)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from repositories.doc import doc_repository
from repositories.chunk import chunk_repository

def verify_parity():
    print("üöÄ D√©marrage V√©rification Parit√© Node.js -> Python...")
    Base.metadata.create_all(bind=engine)
    
    employee_id = "parity_checker"

    # CAS 1 : Test du Post LinkedIn (Doit utiliser PostStrategy)
    post_text = """
    J'ai le plaisir d'annoncer que je rejoins OpenAI en tant que Lead Researcher.
    Apr√®s 5 ans chez Google DeepMind, c'est une nouvelle aventure qui commence !
    #AI #Career #NewJob
    """
    
    print("\n--- [1] Ingestion d'un Post LinkedIn ---")
    res_post = ingestion_service.process_input(
        input_data={"post_text": post_text}, # Format dict pour simuler structure
        employee=employee_id,
        tags=["news"],
        origin="linkedin_import"
    )
    
    # On force la cat√©gorie 'post' car process_input d√©tecte 'raw' par d√©faut pour les dicts
    # Note: Dans une vraie app, le routeur de cat√©gorie ferait ce travail.
    # Ici, pour le test, on va v√©rifier si le 'synthesis' a fonctionn√©.

    # V√©rification Synth√®se
    doc_db = doc_repository.get_doc(res_post['doc_id'], employee_id)
    print(f"üìÑ Doc sauvegard√© : {doc_db.doc}")
    print(f"   Synth√®se g√©n√©r√©e : {doc_db.synthesis[:100]}...") # Doit ne pas √™tre vide
    print(f"   Tags sugg√©r√©s : {doc_db.suggested_tags}")

    if doc_db.synthesis:
        print("‚úÖ SUCC√àS : La synth√®se automatique (synthesis_tags) fonctionne !")
    else:
        print("‚ùå √âCHEC : Pas de synth√®se g√©n√©r√©e.")

    # CAS 2 : V√©rification des m√©tadonn√©es du Chunk
    # On cherche les chunks de ce doc
    chunks = chunk_repository.search("OpenAI", employee_id)
    if chunks and chunks['metadatas'] and len(chunks['metadatas'][0]) > 0:
        meta = chunks['metadatas'][0][0]
        # On v√©rifie si PostStrategy a bien tourn√© (elle ajoute le type 'post')
        print(f"üì¶ Metadata du Chunk : {meta}")
        if meta.get("type") == "post":
            print("‚úÖ SUCC√àS : PostStrategy utilis√©e correctement.")
        else:
            print(f"‚ö†Ô∏è ATTENTION : Type de chunk inattendu ({meta.get('type')}).")

if __name__ == "__main__":
    verify_parity()

# ------------------------------------------
# FICHIER : scripts\verify_entities.py
# ------------------------------------------
import sys
import os
import logging
import json

logging.basicConfig(level=logging.INFO)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from repositories.chunk import chunk_repository

def run_entity_test():
    print("üöÄ Test: Extraction d'Entit√©s (NER)...")
    Base.metadata.create_all(bind=engine)
    
    # Texte riche en entit√©s
    text = """
    Hier, Satya Nadella, le CEO de Microsoft, a annonc√© un partenariat avec Mistral AI.
    L'accord a √©t√© sign√© √† Paris en pr√©sence de Brad Smith.
    Ils vont int√©grer leurs mod√®les dans Azure AI Studio pour concurrencer Google.
    """
    
    doc_id = "test_ner_microsoft"
    employee_id = "tester_ner"

    print(f"\n--- Texte ---\n{text.strip()}\n")

    # Ingestion
    ingestion_service.process_input(
        input_data=text,
        employee=employee_id,
        tags=["tech", "ia"],
        origin="test_ner"
    )

    # V√©rification
    print("\nüîç V√©rification des m√©tadonn√©es du chunk...")
    results = chunk_repository.search("Microsoft", employee_id, limit=1)
    
    if results['metadatas'] and len(results['metadatas'][0]) > 0:
        meta = results['metadatas'][0][0]
        
        print("\nüìä M√©tadonn√©es trouv√©es :")
        print(json.dumps(meta, indent=2))
        
        # Tests
        has_people = "Satya Nadella" in meta.get("entities_people", "")
        has_company = "Microsoft" in meta.get("entities_companies", "")
        has_location = "Paris" in meta.get("entities_locations", "")

        if has_people and has_company:
            print("\n‚úÖ SUCC√àS : Entit√©s 'Satya Nadella' et 'Microsoft' d√©tect√©es !")
        else:
            print("\n‚ö†Ô∏è AVERTISSEMENT : Certaines entit√©s manquent.")
    else:
        print("‚ùå Erreur : Chunk introuvable.")

if __name__ == "__main__":
    run_entity_test()

# ------------------------------------------
# FICHIER : scripts\verify_hypothetical.py
# ------------------------------------------
import sys
import os
import logging

logging.basicConfig(level=logging.INFO)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.connection import engine
from database.models import Base
from services.ingestion import ingestion_service
from repositories.chunk import chunk_repository

def run_test():
    print("üöÄ Test: Chunking avec Questions Hypoth√©tiques...")
    Base.metadata.create_all(bind=engine)
    
    # Texte court mais factuel, propice aux questions
    text = """
    La tour Eiffel est une tour de fer puddl√© de 330 m de hauteur situ√©e √† Paris.
    Construite par Gustave Eiffel pour l'Exposition universelle de Paris de 1889.
    Elle a accueilli plus de 300 millions de visiteurs depuis son ouverture.
    """
    
    doc_id = "test_eiffel_questions"
    employee_id = "tester_hyde"

    # Ingestion
    ingestion_service.process_input(
        input_data=text,
        employee=employee_id,
        tags=["monument", "paris"],
        origin="test_script"
    )

    # V√©rification
    print("\nüîç V√©rification du contenu du chunk...")
    # On r√©cup√®re le dernier chunk ins√©r√© pour ce doc
    results = chunk_repository.search("tour eiffel", employee_id, limit=1)
    
    if results['documents'] and results['documents'][0]:
        content = results['documents'][0][0]
        print("-" * 40)
        print(content)
        print("-" * 40)
        
        if "--- Questions Potentielles ---" in content:
            print("‚úÖ SUCC√àS : Les questions ont √©t√© g√©n√©r√©es et ajout√©es !")
        else:
            print("‚ùå √âCHEC : Pas de questions trouv√©es dans le texte.")
    else:
        print("‚ùå Erreur : Chunk introuvable.")

if __name__ == "__main__":
    run_test()
