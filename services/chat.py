import logging
from typing import List, Tuple

from repositories.prompt import prompt_repository
from services.llm import llm_service
from schemas.chat import ChatRequestNode
# Assurez-vous d'avoir bien ajout√© SearchStrategy dans services/agent_tools.py comme vu pr√©c√©demment
from services.agent_tools import AgentToolExecutor, SearchStrategy 

logger = logging.getLogger(__name__)

class ChatService:
    
    def handle_node_chat(self, request: ChatRequestNode) -> dict:
        """
        Point d'entr√©e principal du Chat.
        Dispatche vers le workflow Expert (avec Prompt) ou Standard.
        """
        logger.info(f"üöÄ [CHAT] Demande re√ßue: Prompt='{request.prompt}' | Q='{request.question}'")

        tools = AgentToolExecutor(request.employee)
        
        # Gestion des exclusions (Toujours exclure les archives par d√©faut)
        exclude = request.exclude if isinstance(request.exclude, dict) else {}
        if "archive" not in exclude.get("categories", []):
            exclude.setdefault("categories", []).append("archive")

        # S√©lection du workflow
        if request.prompt:
            return self._handle_expert_workflow(request, tools, exclude)
        
        return self._handle_standard_workflow(request, tools, exclude)

    def _build_dynamic_context(self, ranked_chunks: List[dict], sort_by: str = "index", max_chunks: int = 0) -> Tuple[str, List[dict]]:
        """
        Construit la cha√Æne de contexte (String) √† partir des chunks bruts.
        
        Args:
            ranked_chunks: Liste des chunks (d√©j√† tri√©s par score/pertinence par smart_retrieve).
            sort_by: 'index' (pour lecture narrative) ou 'score' (pour pertinence pure).
            max_chunks: Limite optionnelle en nombre de chunks (ex: 5 pour du fact-checking).
        """
        selected_chunks = []
        current_chars = 0
        
        # 1. R√©cup√©ration de la limite technique du mod√®le charg√©
        max_chars_limit = llm_service.get_context_limit()
        
        # 2. Application de la limite num√©rique (si demand√©e)
        candidates = ranked_chunks
        if max_chunks > 0:
            candidates = ranked_chunks[:max_chunks]
        
        # 3. Remplissage intelligent (Context Stuffing)
        # On prend les chunks dans l'ordre de pertinence (candidates est tri√© par score)
        # tant qu'il y a de la place en m√©moire.
        for chunk in candidates:
            content_len = len(chunk['content'])
            
            # V√©rification de l'espace disponible (avec marge de s√©curit√©)
            if current_chars + content_len < max_chars_limit:
                selected_chunks.append(chunk)
                current_chars += content_len
            else:
                logger.info(f"üõë Limite contexte atteinte ({current_chars}/{max_chars_limit} chars). Arr√™t.")
                break
        
        if not selected_chunks:
            return "", []

        # 4. Tri Final pour la consommation par le LLM
        if sort_by == "index":
            # Mode NARRATIF (Global) : On remet dans l'ordre du document (Page 1 -> Page 10)
            # C'est crucial pour que le LLM comprenne la structure.
            selected_chunks.sort(key=lambda x: x['metadata'].get('chunk_index', 0))
        else:
            # Mode PERTINENCE (Specific) : On garde les meilleurs scores en premier
            # Utile pour que le LLM voit la r√©ponse la plus probable tout de suite.
            selected_chunks.sort(key=lambda x: x.get('score', 0), reverse=True)

        logger.info(f"üìö Contexte final construit : {len(selected_chunks)} chunks (~{current_chars} chars) | Tri: {sort_by}")

        # 5. Assemblage de la string
        context_str = "\n\n".join([
            f"--- Source: {c['metadata'].get('doc')} (Index: {c['metadata'].get('chunk_index', '?')}, Score: {c.get('score', 0):.2f}) ---\n{c['content']}" 
            for c in selected_chunks
        ])
        
        return context_str, selected_chunks

    def _handle_expert_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        logger.info("üß† [Workflow] D√©marrage mode EXPERT")

        # A. Chargement du Prompt Syst√®me
        prompt_doc = prompt_repository.get_by_name(request.prompt)
        if not prompt_doc:
            return {"response": f"Erreur: Prompt '{request.prompt}' introuvable.", "sources": []}

        target_input = request.question if request.question and request.question.strip() else "Analyse globale"
        
        # B. R√©cup√©ration Intelligente (Router -> Search -> Rerank)
        retrieval_result = tools.smart_retrieve(target_input, request.tags, exclude)
        chunks = retrieval_result["chunks"]
        strategy = retrieval_result["strategy"]

        # C. Construction du Contexte Adaptative
        context_str = ""
        final_selection = []

        if strategy == SearchStrategy.GLOBAL:
            # Strat√©gie R√©sum√© : On prend le max de chunks, tri√©s par Index (lecture livre)
            context_str, final_selection = self._build_dynamic_context(chunks, sort_by="index")
        else:
            # Strat√©gie Pr√©cision : On prend max 5 chunks, tri√©s par Score
            context_str, final_selection = self._build_dynamic_context(chunks, sort_by="score", max_chunks=5)

        # D. Assemblage du Prompt Final
        system_msg = prompt_doc.prompt
        user_msg_content = f"""
        Voici les donn√©es contextuelles r√©cup√©r√©es (Mode: {strategy.value}) :
        
        CONTEXTE:
        {context_str}
        
        INSTRUCTION:
        En utilisant ces donn√©es, ex√©cute la t√¢che demand√©e.
        """
        
        if not final_selection:
            user_msg_content = f"Aucune donn√©e pertinente trouv√©e pour '{target_input}'. Fais de ton mieux avec tes connaissances g√©n√©rales."

        # Historique (si pr√©sent)
        history_block = ""
        if request.history:
            history_block = "HISTORIQUE:\n" + "\n".join(
                [f"{m.get('role','').upper()}: {m.get('content','')}" for m in request.history]
            ) + "\n\n"

        full_prompt = f"{system_msg}\n\n{history_block}USER TASK:\n{user_msg_content}"
        
        # E. G√©n√©ration
        try:
            response = llm_service.generate_response(
                system_prompt="Tu es un expert qualifi√©.",
                user_input=full_prompt
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration : {e}")
            return {"response": "D√©sol√©, une erreur technique est survenue lors de la g√©n√©ration.", "sources": []}

        # Formatage des sources pour le frontend
        unique_sources = {c['metadata'].get('doc'): c for c in final_selection}.values()
        sources_list = [{"name": c['metadata'].get('doc'), "score": c.get("score")} for c in unique_sources]

        return {"response": response, "sources": sources_list}

    def _handle_standard_workflow(self, request: ChatRequestNode, tools: AgentToolExecutor, exclude: dict) -> dict:
        """
        Workflow Chat standard (sans prompt expert pr√©d√©fini).
        """
        query = request.question if request.question.strip() else "R√©sum√©"
        
        # A. R√©cup√©ration Intelligente
        retrieval_result = tools.smart_retrieve(query, request.tags, exclude)
        chunks = retrieval_result["chunks"]
        strategy = retrieval_result["strategy"]
        
        # B. Contexte Adaptatif
        if strategy == SearchStrategy.GLOBAL:
            context_str, final_selection = self._build_dynamic_context(chunks, sort_by="index")
        else:
            context_str, final_selection = self._build_dynamic_context(chunks, sort_by="score", max_chunks=5)
        
        # C. G√©n√©ration
        answer = llm_service.generate_response(
            system_prompt="Tu es un assistant utile et pr√©cis. R√©ponds en fran√ßais en te basant sur le contexte fourni.",
            user_input=query,
            context=context_str
        )
        
        unique_sources = {c['metadata'].get('doc'): c for c in final_selection}.values()
        sources_list = [{"name": c['metadata'].get('doc'), "score": c.get("score")} for c in unique_sources]
        
        return {"response": answer, "sources": sources_list}

chat_service = ChatService()